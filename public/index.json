[
{
	"uri": "https://sagemaker-workshop.com/",
	"title": "Amazon SageMaker Workshop",
	"tags": [],
	"description": "",
	"content": "Amazon SageMaker Workshop Amazon SageMaker is a fully managed service that enables developers and data scientists to quickly and easily build, train, and deploy machine learning models at any scale. This workshop will guide you through using the numerous features of SageMaker.\nYou\u0026rsquo;ll start by creating a SageMaker notebook instance with the required permissions. You will then interact with SageMaker via sample Jupyter notebooks, the AWS CLI, the SageMaker console, or all three. During the workshop, you\u0026rsquo;ll explore various data sets, create model training jobs using SageMaker\u0026rsquo;s hosted training feature, and create endpoints to serve predictions from your models using SageMaker\u0026rsquo;s hosted endpoint feature.\n"
},
{
	"uri": "https://sagemaker-workshop.com/cleanup/sagemaker.html",
	"title": "SageMaker Resources",
	"tags": [],
	"description": "",
	"content": "To avoid charges for resources you no longer need when you\u0026rsquo;re done with this workshop, you can delete them or, in the case of your notebook instance, stop them. Here are the resources you should consider:\nEndpoints: these are the clusters of one or more instances serving inferences from your models. If you did not delete them from within a notebook, you can delete them via the SageMaker console. To do so:\nClick the Endpoints link in the left panel.\nThen, for each endpoint, click the radio button next to it, then select Delete from the Actions drop down menu.\nYou can follow a similar procedure to delete the related Models and Endpoint configurations.\nNotebook instance: you have two options if you do not want to keep the notebook instance running. If you would like to save it for later, you can stop rather than deleting it.\nTo stop a notebook instance: click the Notebook instances link in the left pane of the SageMaker console home page. Next, click the Stop link under the \u0026lsquo;Actions\u0026rsquo; column to the left of your notebook instance\u0026rsquo;s name. After the notebook instance is stopped, you can start it again by clicking the Start link. Keep in mind that if you stop rather than delete it, you will be charged for the storage associated with it.\nTo delete a notebook instance: first stop it per the instruction above. Next, click the radio button next to your notebook instance, then select Delete from the Actions drop down menu.\nS3 Bucket: if you retain the S3 bucket created for this workshop, you will be charged for storage. To avoid these charges if you no longer wish to use the bucket, you may delete it. To delete the bucket, go to the S3 service console, and locate your bucket\u0026rsquo;s name in the bucket table. Next, click in the bucket table row for your bucket to highlight the table row. At the top of the table, the Delete Bucket button should now be enabled, so click it and then click the Confirm button in the resulting pop-up to complete the deletion.\n"
},
{
	"uri": "https://sagemaker-workshop.com/security_for_sysops/scenario.html",
	"title": "Scenario",
	"tags": [],
	"description": "",
	"content": "You are a member of a Cloud Platform Engineering team that has been tasked with enabling your business\u0026rsquo;s data scientists to deliver machine learning-based projects that are trained on highly sensitive company and customer data. The project teams are constrained by shared on-premise resources so you have been tasked with determining how the business can leverage the cloud to provision environments for the data science teams. The environment must be secure, protecting the sensitive data, while also enabling the data science teams to self-service.\nDuring this series of labs, you will be creating a secure environment for a team of data scientists with self-service tools to manage the environment and deliver an ML project.\nThere are three roles involved across these labs:\nCloud Platform Engineering - responsible for managing the cloud environments Project Administrators - responsible for managing resources to support the data science teams and their projects Data Scientist - a member of a project team tasked with delivering an ML or data science project These 3 roles will work together to create a secure cloud environment with appropriate guard rails, provision a secure data science environment, and deliver an ML project working with sensitive data. You will start as a member of the cloud platform engineering team to define a secure mechanism for delivering resources on demand at the request of the project administration team. Then, as a project administrator, you will use this mechanism to provide the data science team with a way to request the tools they need to deliver their project. Finally, as a member of the data science team, you will use the mechanisms provided to self-service and provision a Jupyter Notebook server. Using that server you will then develop and train a model while exploring the security controls in the data science environment.\nTo do this, the roles will work together to configure environments and iterate to improve the security posture across 5 labs.\nLab 1: Deploy the base infrastructure\nAs the cloud platform engineer, create a shared service VPC to host a PyPI mirror for approved Python packages and an AWS Service Catalog portfolio to provide a self-service mechanism to the project administration team.\nLab 2: Deploy the a project team\u0026rsquo;s resources\nAs a project administrator, deploy a project-specific data science environment that protects against data exfiltration using a VPC with no Internet connectivity. Restrict access to this environment using an IAM role for the data science project team, and provide a project-specific self-service mechanism using a Service Catalog portfolio so the data scientists can obtain just-in-time on-demand resources.\nLab 3: Deploy an Amazon SageMaker notebook\nAs a data scientist, use your project\u0026rsquo;s Service Catalog to deploy an Amazon SageMaker notebook.\nLab 4: Create a training job in line with security policy\nAs a data scientist, observe your training job\u0026rsquo;s performance as security controls respond to incorrect configuration parameters.\nLab 5: Improve security controls\nAs the project administrator, alter the IAM policies governing the data science environment to deliver preventive controls to guard your sensitive data.\nNext, let\u0026rsquo;s review the tools you\u0026rsquo;ll need to complete these labs.\n"
},
{
	"uri": "https://sagemaker-workshop.com/security_for_users/scenario.html",
	"title": "Scenario",
	"tags": [],
	"description": "",
	"content": "You are a data scientist or ML engineer who works at a company that wishes to enable their data scientists to deliver machine learning-based projects that are trained on highly sensitive company data. The project teams are constrained by shared on-premise resources so your sysops admins have created all the infrastructure-as-code templates needed to provision a secure environment, which protects the sensitive data while also enabling the data science teams to self-service.\nYou as a data scientist or engineer will need to quickly set this environment up for yourself, so you can start working on exploring your data, training, deploying and monitoring your models. The key takeaways of this workshop are:\nCompute and Network Isolation\nDeploy Amazon SageMaker in a secure, customer-managed VPC.\nAuthentication and Authorization\nProvide single user access to Jupyter over IAM.\nArtifact Management\nEnable private Git integration, lifecycle config, and versioning.\nData Encryption\nEncrypt data in motion and at rest across the entire ML workflow.\nTraceability and Auditability\nTrace model lineage, and audit all API calls and data events.\nExplainability and Interpretability\nExplain predictions with feature importance and SHAP values.\nReal-time Model Monitoring\nMonitor the performance of a productionized model.\nReproducibility\nReproduce the model and results based on saved artifacts.\nIn these notebooks you will see some recommended practices on how to implement these requirements using Amazon SageMaker. Note that while these are recommended practices and guidelines, the information included in these notebooks is for illustrative purposes only. Nothing in this notebook is intended to provide you legal, compliance, or regulatory guidance.\nThe specific features and functionalities that you will become familiar with are:\nImporting custom libraries using pip without having public internet connectivity\nTraining a model with and without VPC and implementation of preventative controls to avoid training without VPC.\nImporting networking configurations, KMS keys directly in the notebook without data scientist having to know what they are.\nUsing SageMaker Processing to run scikit-learn data pre-processing jobs in Python.\nUsing SageMaker training on spot instances to save on cost\nModel Explainability using SHAP\nPushing/pulling code to private AWS CodeCommit Repository\nDeploying a trained model and monitoring it for data drift with SageMaker ModelMonitor\nSecurely running training, processing jobs using KMS keys to ensure encryption at rest and PrivateLink to support encryption in transit.\nUsing SageMaker Experiments to maintain lineage and traceability of model artifacts.\nIn the following labs you will quickly provision a collection of mechanisms and guard rails to enable you, as a project team member, to provisoin infrastructure and tools to support your project. You will then work through the machine learning lifecycle in the secure environment to see how project teams can be enabled to work in an agile manner at speed.\n"
},
{
	"uri": "https://sagemaker-workshop.com/custom/code.html",
	"title": "Submit custom code",
	"tags": [],
	"description": "",
	"content": "In this section, you will train a neural network locally on the location from where this notebook is run (typically the SageMaker Notebook instance) using MXNet. You will then see how to create an endpoint from the trained MXNet model and deploy it on SageMaker. You will then inference from the newly created SageMaker endpoint. For this section, you\u0026rsquo;ll be using the MNIST dataset.\nRunning the notebook Download the mxnet_mnist_byom.zip file. This archive contains the Python script to train your model and the Jupyter notebook to step through the process. Unzip this on your local environment.\nAccess the SageMaker notebook instance you created earlier. Click the New button on the right and select Folder.\nClick the checkbox next to your new folder, click the Rename button above in the menu bar, and give the folder a name such as \u0026lsquo;mxnet-mnist-byom\u0026rsquo;.\nClick the folder to enter it.\nTo upload the notebook, click the Upload button on the right. Then in the file selection popup, select the file \u0026lsquo;mxnet_mnist_byom.ipynb\u0026rsquo; from the folder on your computer where you downloadeded it earlier. Click the blue Upload button that appears to the right of the notebook\u0026rsquo;s file name. Repeat this step for the \u0026lsquo;mnist.py\u0026rsquo; and the \u0026lsquo;input.html\u0026rsquo; files.\nYou are now ready to begin the notebook. Click the notebook\u0026rsquo;s file name to open it.\nFollow the directions in the notebook. The notebook will walk you through the data preparation, training, hosting, and validating the model with Amazon SageMaker. Once completed, return from the notebook to these instructions to move to the next Module.\n"
},
{
	"uri": "https://sagemaker-workshop.com/builtin/xgboost.html",
	"title": "Video Game Sales Prediction with XGBoost",
	"tags": [],
	"description": "",
	"content": "In this section, you\u0026rsquo;ll work your way through a Jupyter notebook that demonstrates how to use a built-in algorithm in SageMaker. More specifically, we\u0026rsquo;ll use SageMaker\u0026rsquo;s version of XGBoost, a popular and efficient open-source implementation of the gradient boosted trees algorithm.\nGradient boosting is a supervised learning algorithm that attempts to predict a target variable by combining the estimates of a set of simpler, weaker models. XGBoost has done remarkably well in machine learning competitions because it robustly handles a wide variety of data types, relationships, and distributions. It often is a useful, go-to algorithm in working with structured data, such as data that might be found in relational databases and flat files.\nThis section also shows how to use SageMaker\u0026rsquo;s built-in algorithms via hosted Jupyter notebooks, the AWS CLI, and the SageMaker console.\nExploratory Data Analysis Download the video-game-sales-xgboost.ipynb notebook.\nAccess the SageMaker notebook instance you created earlier. Click the New button on the right and select Folder.\nClick the checkbox next to your new folder, click the Rename button above in the menu bar, and give the folder a name such as \u0026lsquo;video-game-sales\u0026rsquo;.\nClick the folder to enter it.\nTo upload the notebook, click the Upload button on the right. Then in the file selection popup, select the file \u0026lsquo;video-game-sales-xgboost.ipynb\u0026rsquo; from the folder on your computer where you downloadeded it earlier. Click the blue Upload button that appears to the right of the notebook\u0026rsquo;s file name.\nYou are now ready to begin the notebook. Click the notebook\u0026rsquo;s file name to open it.\nIn the bucket = '\u0026lt;your_s3_bucket_name_here\u0026gt;' code line, paste the name of the S3 bucket you created in Module 1 to replace \u0026lt;your_s3_bucket_name_here\u0026gt;. The code line should now read similar to bucket = 'smworkshop-john-smith'. Do NOT paste the entire path (s3://\u0026hellip;\u0026hellip;.), just the bucket name.\nFollow the directions in the notebook. When it is time to set up a training job, return from the notebook to these instructions.\nModel Training Now that you have your data in S3, you can begin training a model. You\u0026rsquo;ll use SageMaker\u0026rsquo;s built-in version of the XGBoost algorithm, and the AWS CLI to run the training job. XGBoost has many tunable hyperparameters. Some of these hyperparameters are listed below; initially we\u0026rsquo;ll only use a few of them. Many of the hyperparameters are used to prevent overfitting, which prevents a model from generalizing to new observations. max_depth: Maximum depth of a tree. As a cautionary note, a value too small could underfit the data, while increasing it will make the model more complex and thus more likely to overfit the data (in other words, the classic bias-variance tradeoff).\neta: Step size shrinkage used in updates to prevent overfitting.\neval_metric: Evaluation metric(s) for validation data. For data sets such as this one with imbalanced classes, we\u0026rsquo;ll use the AUC metric.\nscale_pos_weight: Controls the balance of positive and negative weights, again useful for data sets having imbalanced classes.\nYou\u0026rsquo;ll be using the AWS CLI and a Bash script to run the training job. Using the AWS CLI and scripts is an excellent way to automate machine learning pipelines and repetitive tasks, such as periodic training jobs. As a reminder, in the Introduction module we recommended the use of AWS Cloud 9 for access to the AWS CLI and Bash environments. If you haven\u0026rsquo;t done so already, please set up and open your Cloud9 environment now as described in Cloud9 Setup. Below is a screenshot of what your Cloud9 environment should look like as you create the first script below and run the related commands.\nCreate a text file named videogames.sh. If you haven\u0026rsquo;t done so already, open a terminal/command window that supports Bash to enter commands. In the terminal window, change to the directory in which you created the file (if you\u0026rsquo;re not already there), then run the following command:\nchmod +x videogames.sh Paste the bash script below into the videogames.sh file, and then change the text in the angle brackets (\u0026lt; \u0026gt;) as follows. Do NOT put quotes around the values you insert, or retain the brackets.\narn_role: To get the value for this variable, go to the SageMaker console, click Notebook instances in the left pane, then in the \u0026lsquo;Notebook instances\u0026rsquo; table, click the name of the instance you created for this workshop. In the Notebook instance settings section, look for the \u0026lsquo;IAM role ARN\u0026rsquo; value, and copy its text. It should look like the following: arn:aws:iam::1234567890:role/service-role/AmazonSageMaker-ExecutionRole-20171211T211964.\ntraining_image: select one of the following, depending on the AWS Region where you are running this workshop.\nN. Virginia: 811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest Oregon: 433757028032.dkr.ecr.us-west-2.amazonaws.com/xgboost:latest Ohio: 825641698319.dkr.ecr.us-east-2.amazonaws.com/xgboost:latest Ireland: 685385470294.dkr.ecr.eu-west-1.amazonaws.com/xgboost:latest bucket: the name of the S3 bucket you used in your notebook. It should look like: s3://smworkshop-john-smith.\nregion: the region code for the region where you are running this workshop, either us-east-1 for N. Virginia, us-west-2 for Oregon, us-east-2 for Ohio, or eu-west-1 for Ireland.\n# Fill in the values of these four variables arn_role=\u0026lt;arn-of-your-notebook-role\u0026gt; training_image=\u0026lt;training-image-for-region\u0026gt; bucket=\u0026lt;name-of-your-s3-bucket\u0026gt; region=\u0026lt;your-region\u0026gt; prefix=/sagemaker/videogames_xgboost training_job_name=videogames-xgboost-`date \u0026#39;+%Y-%m-%d-%H-%M-%S\u0026#39;` training_data=$bucket$prefix/train eval_data=$bucket$prefix/validation train_source={S3DataSource={S3DataType=S3Prefix,S3DataDistributionType=FullyReplicated,S3Uri=$training_data}} eval_source={S3DataSource={S3DataType=S3Prefix,S3DataDistributionType=FullyReplicated,S3Uri=$eval_data}} aws --region $region \\ sagemaker create-training-job \\ --role-arn $arn_role \\ --training-job-name $training_job_name \\ --algorithm-specification TrainingImage=$training_image,TrainingInputMode=File \\ --resource-config InstanceCount=1,InstanceType=ml.c4.2xlarge,VolumeSizeInGB=10 \\ --input-data-config ChannelName=train,DataSource=$train_source,CompressionType=None,ContentType=libsvm ChannelName=validation,DataSource=$eval_source,CompressionType=None,ContentType=libsvm \\ --output-data-config S3OutputPath=$bucket$prefix \\ --hyper-parameters max_depth=3,eta=0.1,eval_metric=auc,scale_pos_weight=2.0,subsample=0.5,objective=binary:logistic,num_round=100 \\ --stopping-condition MaxRuntimeInSeconds=1800 In your terminal window, run the following command to start the training job. Total job duration may last up to about 5 minutes, including time for setting up the training cluster. In case the training job encounters problems and is stuck, you can set a stopping condition that times out, in this case after a half hour. ``` ./videogames.sh ``` In the SageMaker console, click Jobs in the left panel to check the status of the training job. When the job is complete, its Status column will change from InProgress to Complete. As a reminder, duration of this job can last up to about 5 minutes, including time for setting up the training cluster. \u0026gt; To check the actual training time (not including cluster setup) for a job when it is complete, click the training job name in the jobs table, then examine the **Training time** listed at the top right under **Job Settings**. Model Creation Now that you\u0026rsquo;ve trained your machine learning model, you\u0026rsquo;ll want to make predictions by setting up a hosted endpoint for it. The first step in doing that is to create a SageMaker model object that wraps the actual model artifact from training. To create the model object, you will point to the model.tar.gz that came from training and the inference code container, then create the hosting model object. Here are the steps to do this via the SageMaker console (see screenshot below for an example of all relevant fields filled in for the Oregon AWS Region): In the left pane of the SageMaker console home page, right click the Models link and open it in another tab of your browser. Click the Create Model button at the upper right above the \u0026lsquo;Models\u0026rsquo; table.\nFor the \u0026lsquo;Model name\u0026rsquo; field under Model Settings, enter videogames-xgboost.\nFor the \u0026lsquo;Location of inference code image\u0026rsquo; field under Primary Container, enter the name of the same Docker image you specified previously for the region where you\u0026rsquo;re running this workshop. For ease of reference, here are the image names again:\nN. Virginia: 811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest Oregon: 433757028032.dkr.ecr.us-west-2.amazonaws.com/xgboost:latest Ohio: 825641698319.dkr.ecr.us-east-2.amazonaws.com/xgboost:latest Ireland: 685385470294.dkr.ecr.eu-west-1.amazonaws.com/xgboost:latest For the \u0026lsquo;Location of model artifacts\u0026rsquo; field under Primary Container, enter the path to the output of your replicated training job. To find the path, go back to your first browser tab, click Jobs in the left pane, then find and click the replicated job name, which will look like videogames-xgboost-\u0026lt;date\u0026gt;. Scroll down to the Outputs section, then copy the path under \u0026lsquo;S3 model artifact\u0026rsquo;. Paste the path in the field; it should look like s3://smworkshop-john-smith/sagemaker/videogames_xgboost/videogames-xgboost-2018-04-17-20-40-13/output/model.tar.gz .\nClick Create model at the bottom of the page.\nModel Serving and Evaluation Once you\u0026rsquo;ve created your model, you can configure what your hosting endpoint should be. Here you specify the EC2 instance type to use for hosting, the initial number of instances, and your hosting model name. Here are the steps to do this via the SageMaker console (see screenshot below for an example of all relevant fields filled in for the Oregon AWS Region): In the left pane of the SageMaker console, click Endpoint configuration. Click the Create endpoint configuration button at the upper right above the \u0026lsquo;Endpoint configuration\u0026rsquo; table.\nFor the \u0026lsquo;Endpoint configuration name\u0026rsquo; field under New endpoint configuration, enter videogames-xgboost.\nUnder Production variants, click Add model. From the Add model popup, select the videogames-xgboost model you created earlier, and click Save. Then click Create endpoint configuration at the bottom of the page.\nFor the final step in the process of setting up an endpoint, you\u0026rsquo;ll use the SageMaker console to do so (see screenshot below for an example of all relevant fields filled in for the Oregon AWS Region): In the left pane of the SageMaker console, click Endpoints. Click the Create endpoint button at the upper right above the \u0026lsquo;Endpoints\u0026rsquo; table.\nFor the \u0026lsquo;Endpoint name\u0026rsquo; field under Endpoint, enter videogames-xgboost.\nUnder Attach endpoint configuration, leave \u0026lsquo;Use an existing endpoint configuration\u0026rsquo; selected, then under Endpoint configuration, select videogames-xgboost from the table, then click Select endpoint configuration at the bottom of the table. Then click Create endpoint at the bottom of the page.\nIn the Endpoints table, refer to the \u0026lsquo;Status\u0026rsquo; column, and wait for the endpoint status to change from \u0026lsquo;Creating\u0026rsquo; to \u0026lsquo;InService\u0026rsquo; before proceeding to the next step. It will take several minutes for endpoint creation, possibly as long as ten minutes.\nTo evaluate predictions from your model, let\u0026rsquo;s return to the notebook you used earlier. When you are finished, return here and proceed to the next section. Conclusion \u0026amp; Extensions This XGBoost model is just the starting point for predicting whether a game will be a hit based on reviews and other features. There are several possible avenues for improving the model\u0026rsquo;s performance. First, of course, would be to collect more data and, if possible, fill in the existing missing fields with actual information. Another possibility is further hyperparameter tuning, with Amazon SageMaker\u0026rsquo;s Hyperparameter Optimization service. And, although ensemble learners often do well with imbalanced data sets, it could be worth exploring techniques for mitigating imbalances such as downsampling, synthetic data augmentation, and other approaches.\n"
},
{
	"uri": "https://sagemaker-workshop.com/conclusion/conclusion.html",
	"title": "What Have We Accomplished",
	"tags": [],
	"description": "",
	"content": "We have:\nCreated a SageMaker Notebook instance Explored different built-in algorithms Analyzed strategies to distribute data for parallel training Trained a model using a custom script Built and deployed a custom model "
},
{
	"uri": "https://sagemaker-workshop.com/builtin/resnet.html",
	"title": "Image Classification with ResNet",
	"tags": [],
	"description": "",
	"content": "In this section, you\u0026rsquo;ll work your way through a Jupyter notebook that demonstrates how to use a built-in algorithm in SageMaker. More specifically, you\u0026rsquo;ll use SageMaker\u0026rsquo;s image classification algorithm, a supervised learning algorithm that takes an image as input and classifies it into one of multiple output categories. It uses a convolutional neural network (ResNet) that can be trained from scratch, or trained using transfer learning when a large number of training images are not available. In this section you\u0026rsquo;ll train the image classification algorithm from scratch on the Caltech-256 dataset.\nRunning the notebook Download the image-classification-fulltraining.ipynb notebook.\nAccess the SageMaker notebook instance you created earlier. Click the New button on the right and select Folder.\nClick the checkbox next to your new folder, click the Rename button above in the menu bar, and give the folder a name such as \u0026lsquo;image-classification-resnet\u0026rsquo;.\nClick the folder to enter it.\nTo upload the notebook, click the Upload button on the right. Then in the file selection popup, select the file \u0026lsquo;image-classification-fulltraining.ipynb\u0026rsquo; from the folder on your computer where you downloadeded it earlier. Click the blue Upload button that appears to the right of the notebook\u0026rsquo;s file name.\nYou are now ready to begin the notebook. Click the notebook\u0026rsquo;s file name to open it.\nIn the bucket = '\u0026lt;\u0026lt;bucket_name\u0026gt;\u0026gt;' code line, paste the name of the S3 bucket you created in Module 1 to replace \u0026lt;\u0026lt;bucket_name\u0026gt;\u0026gt;. The code line should now read similar to bucket = 'smworkshop-john-smith'. Do NOT paste the entire path (s3://\u0026hellip;\u0026hellip;.), just the bucket name.\nFollow the directions in the notebook. The notebook will walk you through the data preparation, training, hosting, and validating the model with Amazon SageMaker. Once completed, return from the notebook to these instructions to move to the next Module.\n"
},
{
	"uri": "https://sagemaker-workshop.com/security_for_sysops/tools.html",
	"title": "Tools",
	"tags": [],
	"description": "",
	"content": "To work through these labs you will need:\nAn AWS account\nWith privileges to create IAM roles, attach IAM policies, create AWS VPCs, configure Service Catalog, create Amazon S3 buckets, and work with Amazon SageMaker.\nAccess to the AWS web console\nMany of the instructions will guide you through working with the various service consoles.\nJupyter cheat sheet\nIf you are unfamiliar with the Jupyter notebook interface or its keybindings a cheat sheet may help you navigate.\nOptional: AWS CLI\nYou may want to have the AWS CLI available for working with AWS services like Amazon S3.\nNow, let\u0026rsquo;s get started!\n"
},
{
	"uri": "https://sagemaker-workshop.com/security_for_users/tools.html",
	"title": "Tools &amp; Knowledge Check",
	"tags": [],
	"description": "",
	"content": "To work through these labs you will need:\nAn AWS account\nWith privileges to create IAM roles, attach IAM policies, create AWS VPCs, configure Service Catalog, create Amazon S3 buckets, and work with Amazon SageMaker.\nAccess to the AWS web console\nMany of the instructions will guide you through working with the various service consoles.\nTo get the most of these labs it will be beneficial if you have prior experience working with the following technologies:\nPython\nPython is a programming language that is popular in data science communities. It has been used in the labs to work with the AWS services and the data being used to train machine learning models.\nGit\nThe labs make use of Git to manage the work you will complete. Git is a distributed version control system and you will use a few simple commands to interact with a Git repository during the labs.\nSageMaker SDK\nSageMaker SDK is a high-level Python SDK wrapped around Boto3 and designed to provide a familiar interface to data science users.\nBoto3\nBoto3 is a low-level Python SDK for interacting with the AWS APIs. Documentation on its many great classes and functionality can be found online\nPandas\nThe notebooks use Pandas in many different places to load, export, and manipulate data. If you are unfamiliar with the Pandas library it may be helpful to review some of their Getting Started materials.\nscikit-learn\nscikit-learn is a popular open source framework for data science and machine learning.\nXGBoost\nxgboost is one of the most popular and performant gradient boosting algorithms for supervised learning tasks.\nJupyter\nJupyter is a popular open source interactive computing environment with a user friendly notebook interface.\nYou will use Jupyter notebooks to complete these labs. If you have not used Jupyter before you may find a Jupyter cheat sheet to be useful. The cheat sheet walks through navigation of the Jupyter interface and how to use a notebook.\nSageMaker Experiments\nSageMaker Experiments APIs can be used to manage and track the metadata for your training, pre-processing, hyperparameter tuning jobs.\nSageMaker Model Monitor\nSageMaker ModelMonitor can be used to detect data drift during inference time on the payload sent to your endpoint. It can be connected to CloudWatch to send an alarm or notification when violations are detected and users need to be alerted.\nSageMaker Processing\nSageMaker Processing can be used to run your scripts for pre-processing, feature engineering in a managed way where Amazon SageMaker sets up the underlying infrastructure needed to run your job at scale, tearing down the instances once the job is complete.\nNow, let\u0026rsquo;s get started!\n"
},
{
	"uri": "https://sagemaker-workshop.com/builtin/rcf.html",
	"title": "Anomaly Detection with Random Cut Forest",
	"tags": [],
	"description": "",
	"content": "In this section, you\u0026rsquo;ll work your way through a Jupyter notebook that demonstrates how to use a built-in algorithm in SageMaker. More specifically, you\u0026rsquo;ll use SageMaker\u0026rsquo;s Random Cut Forest (RCF) algorithm, an algorithm designed to detect anomalous data points within a dataset. Examples of when anomalies are important to detect include when website activity uncharacteristically spikes, when temperature data diverges from a periodic behaviour, or when changes to public transit ridership reflect the occurrence of a special event.\nIn this notebook, we will use the SageMaker RCF algorithm to train a model on the Numenta Anomaly Benchmark (NAB) NYC Taxi dataset which records the amount New York City taxi ridership over the course of six months. We will then use this model to predict anomalous events by emitting an \u0026ldquo;anomaly score\u0026rdquo; for each data point.\nRunning the notebook Access the SageMaker notebook instance you created earlier. Open the SageMaker Examples tab.\nIn the Introduction to Amazon Algorithms section locate the random_cut_forest.ipynb notebook and create a copy by clicking on Use.\nYou are now ready to begin the notebook.\nIn the bucket = '\u0026lt;\u0026lt;bucket_name\u0026gt;\u0026gt;' code line, paste the name of the S3 bucket you created in Module 1 to replace \u0026lt;\u0026lt;bucket_name\u0026gt;\u0026gt;. The code line should now read similar to bucket = 'smworkshop-john-smith'. Do NOT paste the entire path (s3://\u0026hellip;\u0026hellip;.), just the bucket name.\nFollow the directions in the notebook. The notebook will walk you through the data preparation, training, hosting, and validating the model with Amazon SageMaker. Once completed, return from the notebook to these instructions to move to the next Module.\n"
},
{
	"uri": "https://sagemaker-workshop.com/custom/algo.html",
	"title": "Use your own custom algorithms",
	"tags": [],
	"description": "",
	"content": "In this section, you\u0026rsquo;ll create your own training script using TensorFlow and the building blocks provided in tf.layers, which will predict the ages of abalones based on their physical measurements. It\u0026rsquo;s possible to estimate the age of an abalone (sea snail) by the number of rings on its shell. In this section, you\u0026rsquo;ll be using the UCI Abalone dataset.\nWriting Custom TensorFlow Model Training and Inference Code To train a model on Amazon SageMaker using custom TensorFlow code and deploy it on Amazon SageMaker, you need to implement training and inference code interfaces in your code.\nYour TensorFlow training script must be a Python 2.7 source file. The current default TensorFlow version is 1.6. This training/inference script must contain the following functions:\nmodel_fn: Defines the model that will be trained. train_input_fn: Preprocess and load training data. eval_input_fn: Preprocess and load evaluation data. serving_input_fn: Defines the features to be passed to the model during prediction. For more information, see TensorFlow Model Training Code in the Amazon SageMaker documentation.\nDefining the model The model_fn is a function that contains all the logic to support training, evaluation, and prediction. The basic skeleton for a model_fn looks like this:\ndef model_fn(features, labels, mode, hyperparameters): # Logic to do the following: # 1. Configure the model via TensorFlow operations # 2. Define the loss function for training/evaluation # 3. Define the training operation/optimizer # 4. Generate predictions # 5. Return predictions/loss/train_op/eval_metric_ops in EstimatorSpec object return EstimatorSpec(mode, predictions, loss, train_op, eval_metric_ops) The model_fn function must accept four positional arguments:\nfeatures: A dict containing the features passed to the model via train_input_fn in training mode, via eval_input_fn in evaluation mode, and via serving_input_fn in predict mode. labels: A Tensor containing the labels passed to the model via train_input_fn in training mode and eval_input_fn in evaluation mode. It will be empty for predict mode. mode: One of the following tf.estimator.ModeKeys string values indicating the context in which the model_fn was invoked: TRAIN: the model_fn was invoked in training mode. EVAL: the model_fn was invoked in evaluation mode. PREDICT: the model_fn was invoked in predict mode. hyperparameters: The hyperparameters passed to SageMaker TrainingJob that runs your TensorFlow training script. You can use this to pass hyperparameters to your training script. The model_fn function must return a tf.estimator.EstimatorSpec.\nMore details on how to create a model_fn can be find in Constructing the model_fn.\nTraining and Evaluation The train_input_fn function is used to pass features and labels to the model_fn in training mode. The eval_input_fn function is used to features and labels to the model_fn in evaluation mode.\nThe basic skeleton for the train_input_fn looks like this:\ndef train_input_fn(training_dir, hyperparameters): # Logic to the following: # 1. Reads the **training** dataset files located in training_dir # 2. Preprocess the dataset # 3. Return 1) a dict of feature names to Tensors with # the corresponding feature data, and 2) a Tensor containing labels return features, labels An eval_input_fn follows the same format:\ndef eval_input_fn(training_dir, hyperparameters): # Logic to the following: # 1. Reads the **evaluation** dataset files located in training_dir # 2. Preprocess the dataset # 3. Return 1) a dict of feature names to Tensors with # the corresponding feature data, and 2) a Tensor containing labels return features, labels Note: For TensorFlow 1.4 and 1.5, train_input_fn and eval_input_fn may also return a no-argument function which returns the tuple features, labels. This is no longer supported for TensorFlow 1.6 and up.\nMore details on how to create input functions can be find in Building Input Functions with tf.estimator.\nServing the Model The serving_input_fn function is used to define the shapes and types of the inputs the model accepts when the model is exported for Tensorflow Serving. It is optional, but required for deploying the trained model to a SageMaker endpoint.\nThe serving_input_fn function is called at the end of model training and is not called during inference.\nThe basic skeleton for the serving_input_fn looks like this:\ndef serving_input_fn(hyperparameters): # Logic to the following: # 1. Defines placeholders that TensorFlow serving will feed with inference requests # 2. Preprocess input data # 3. Returns a tf.estimator.export.ServingInputReceiver or tf.estimator.export.TensorServingInputReceiver, # which packages the placeholders and the resulting feature Tensors together. Note: For TensorFlow 1.4 and 1.5, serving_input_fn may also return a no-argument function which returns a tf.estimator.export.ServingInputReceiver ortf.estimator.export.TensorServingInputReceiver. This is no longer supported for TensorFlow 1.6 and up.\nMore details on how to create a serving_input_fn can be find in Preparing serving inputs.\nRunning the notebook Download the tensorflow_abalone_age_predictor.zip file. This archive contains the Python script for your model, the Jupyter notebook to step through the process, and the UCI Abalone dataset.\nAccess the SageMaker notebook instance you created earlier. Click the New button on the right and select Folder.\nClick the checkbox next to your new folder, click the Rename button above in the menu bar, and give the folder a name such as \u0026lsquo;tensorflow-abalone-byom\u0026rsquo;.\nClick the folder to enter it.\nClick the Upload button on the right. Then in the file selection popup, select the file \u0026lsquo;tensorflow_abalone_age_predictor.zip\u0026rsquo; from the folder on your computer where you downloadeded it earlier. Click the blue Upload button that appears to the right of the notebook\u0026rsquo;s file name.\nClick the New button on the right and select Terminal. In the terminal window, type the following commands to unzip the archive:\ncd SageMaker unzip tensorflow_abalone_age_predictor.zip Close the Terminal window. You are now ready to begin the notebook. Click the notebook\u0026rsquo;s file name to open it.\nFollow the directions in the notebook. The notebook will walk you through the data preparation, training, hosting, and validating the model with Amazon SageMaker. Once completed, return from the notebook to these instructions to move to the next Module.\n"
},
{
	"uri": "https://sagemaker-workshop.com/builtin/parallelized.html",
	"title": "Parallelized Data Distribution",
	"tags": [],
	"description": "",
	"content": "SageMaker makes it easy to train machine learning models across a cluster containing a large number of machines. This a non-trivial process, but SageMaker\u0026rsquo;s built-in algorithms and pre-built MXNet and TensorFlow containers hide most of the complexity from you. Nevertheless, there are decisions about how to structure data that will have implications regarding how the distributed training is carried out.\nIn this section, you will learn about how to take full advantage of distributed training clusters when using one of SageMaker\u0026rsquo;s built-in algorithms. This section also shows how to use SageMaker\u0026rsquo;s built-in algorithms via hosted Jupyter notebooks, the AWS CLI, and the SageMaker console.\nExploratory Data Analysis Download the data_distribution_types.ipynb notebook.\nAccess the SageMaker notebook instance you created earlier. Click the New button on the right and select Folder.\nClick the checkbox next to your new folder, click the Rename button above in the menu bar, and give the folder a name such as \u0026lsquo;distributed-data\u0026rsquo;.\nClick the folder to enter it.\nTo upload the notebook, click the Upload button on the right. Then in the file selection popup, select the file \u0026lsquo;data_distribution_types.ipynb\u0026rsquo; from the folder on your computer where you downloadeded it earlier. Click the blue Upload button that appears to the right of the notebook\u0026rsquo;s file name.\nYou are now ready to begin the notebook. Click the notebook\u0026rsquo;s file name to open it.\nIn the bucket = '\u0026lt;your_s3_bucket_name_here\u0026gt;' code line, paste the name of the S3 bucket you created in Module 1 to replace \u0026lt;your_s3_bucket_name_here\u0026gt;. The code line should now read similar to bucket = 'smworkshop-john-smith'. Do NOT paste the entire path (s3://\u0026hellip;\u0026hellip;.), just the bucket name.\nIn this workshop, you also will be accessing a S3 bucket that holds data from one of the AWS Public Data Sets.\nIf you followed the Creating a Notebook Instance module to create your notebook instance, you should be able to access this S3 bucket. Otherwise, if you are using your own notebook instance created elsewhere, you may need to modify the associated IAM role to add permissions for s3:ListBucket for arn:aws:s3:::gdelt-open-data, and s3:GetObject for arn:aws:s3:::gdelt-open-data/*.\nFollow the directions in the notebook. When it is time to set up a training job, return from the notebook to these instructions. Model Training Now that we have our data in S3, you can begin training. You\u0026rsquo;ll use SageMaker\u0026rsquo;s built-in Linear Learner algorithm. Since the focus of this module is data distribution to a training cluster, you\u0026rsquo;ll fit two models in order to compare data distribution types: In the first job, you\u0026rsquo;ll use FullyReplicated for your train channel. This will pass every file in the input S3 location to every machine (in this case you\u0026rsquo;ll be using 5 machines).\nIn the second job, you\u0026rsquo;ll use ShardedByS3Key for the train channel (note that you\u0026rsquo;ll keep FullyReplicated for the validation channel). So, for the training data, you\u0026rsquo;ll pass each S3 object to a separate machine. Since there are 5 files in the dataset (one for each year), you\u0026rsquo;ll train on 5 machines, meaning each machine will get a year\u0026rsquo;s worth of records.\nYou\u0026rsquo;ll be using the AWS CLI and Bash scripts to run the training jobs. Using the AWS CLI and scripts is an excellent way to automate machine learning pipelines and repetitive tasks, such as periodic training jobs. If you haven\u0026rsquo;t done so already, please set up and open your Cloud9 environment now as described in Cloud9 Setup. Below is a screenshot of what your Cloud9 environment should look like as you create the first script below and run the related commands. Step-by-step instructions follow.\nCreate a text file named replicated.sh. If you haven\u0026rsquo;t done so already, open a terminal/command window that supports Bash to enter commands. In the terminal window, change to the directory in which you created the file (if you\u0026rsquo;re not already there), then run the following command:\nchmod +x replicated.sh Paste the bash script below into the replicated.sh file, and then change the text in the angle brackets (\u0026lt; \u0026gt;) as follows. Do NOT put quotes around the values you insert, or retain the brackets.\narn_role: To get the value for this variable, go to the SageMaker console, click Notebook instances in the left pane, then in the \u0026lsquo;Notebook instances\u0026rsquo; table, click the name of the instance you created for this workshop. In the Notebook instance settings section, look for the \u0026lsquo;IAM role ARN\u0026rsquo; value, and copy its text. It should look like the following: arn:aws:iam::1234567890:role/service-role/AmazonSageMaker-ExecutionRole-20171211T211964.\ntraining_image: select one of the following, depending on the AWS Region where you are running this workshop.\nN. Virginia: 382416733822.dkr.ecr.us-east-1.amazonaws.com/linear-learner:latest Oregon: 174872318107.dkr.ecr.us-west-2.amazonaws.com/linear-learner:latest Ohio: 404615174143.dkr.ecr.us-east-2.amazonaws.com/linear-learner:latest Ireland: 438346466558.dkr.ecr.eu-west-1.amazonaws.com/linear-learner:latest bucket: the name of the S3 bucket you used in your notebook. It should look like: s3://smworkshop-john-smith.\nregion: the region code for the region where you are running this workshop, either us-east-1 for N. Virginia, us-west-2 for Oregon, us-east-2 for Ohio, or eu-west-1 for Ireland.\n# Fill in the values of these four variables arn_role=\u0026lt;arn-of-your-notebook-role\u0026gt; training_image=\u0026lt;training-image-for-region\u0026gt; bucket=\u0026lt;name-of-your-s3-bucket\u0026gt; region=\u0026lt;your-region\u0026gt; prefix=/sagemaker/data_distribution_types training_job_name=linear-replicated-`date \u0026#39;+%Y-%m-%d-%H-%M-%S\u0026#39;` training_data=$bucket$prefix/train eval_data=$bucket$prefix/validation train_source={S3DataSource={S3DataType=S3Prefix,S3DataDistributionType=FullyReplicated,S3Uri=$training_data}} eval_source={S3DataSource={S3DataType=S3Prefix,S3DataDistributionType=FullyReplicated,S3Uri=$eval_data}} aws --region $region \\ sagemaker create-training-job \\ --role-arn $arn_role \\ --training-job-name $training_job_name \\ --algorithm-specification TrainingImage=$training_image,TrainingInputMode=File \\ --resource-config InstanceCount=5,InstanceType=ml.c4.2xlarge,VolumeSizeInGB=10 \\ --input-data-config ChannelName=train,DataSource=$train_source,CompressionType=None,RecordWrapperType=None ChannelName=validation,DataSource=$eval_source,CompressionType=None,RecordWrapperType=None \\ --output-data-config S3OutputPath=$bucket$prefix \\ --hyper-parameters feature_dim=25,mini_batch_size=500,predictor_type=regressor,epochs=2,num_models=32,loss=absolute_loss \\ --stopping-condition MaxRuntimeInSeconds=1800 Save your file, then in your terminal window, run the following command to start the training job. Total job duration may last up to about 10 minutes, including time for setting up the training cluster. In case the training job encounters problems and is stuck, you can set a stopping condition that times out, in this case after a half hour. Now, since you can run another job concurrently with this one, move onto the next step after you start this job. ``` ./replicated.sh ``` For the next training job with the ShardedByS3Key distribution type, please create a text file named sharded.sh. then run the following command in your terminal window:\nchmod +x sharded.sh Paste the bash script below into the sharded.sh file, and then change the text in the angle brackets (\u0026lt; \u0026gt;) as follows. Do NOT put quotes around the values you insert, or retain the brackets. All four values to change are the same as the values you changed for the previous script; they are noted again below for your ease of reference.\narn_role: same as for the previous script. It should look like the following: arn:aws:iam::1234567890:role/service-role/AmazonSageMaker-ExecutionRole-20171211T211964.\ntraining_image: same as the previous script; the image depends on the AWS Region where you are running this workshop. They are shown again here for convenience:\nN. Virginia: 382416733822.dkr.ecr.us-east-1.amazonaws.com/linear-learner:latest Oregon: 174872318107.dkr.ecr.us-west-2.amazonaws.com/linear-learner:latest Ohio: 404615174143.dkr.ecr.us-east-2.amazonaws.com/linear-learner:latest Ireland: 438346466558.dkr.ecr.eu-west-1.amazonaws.com/linear-learner:latest bucket: same as for the previous script. It should look like: s3://smworkshop-john-smith.\nregion: the region code for the region where you are running this workshop, either us-east-1 for N. Virginia, us-west-2 for Oregon, us-east-2 for Ohio, or eu-west-1 for Ireland.\n# Fill in the values of these four variables arn_role=\u0026lt;arn-of-your-notebook-role\u0026gt; training_image=\u0026lt;training-image-for-region\u0026gt; bucket=\u0026lt;name-of-your-s3-bucket\u0026gt; region=\u0026lt;your-region\u0026gt; prefix=/sagemaker/data_distribution_types training_job_name=linear-sharded-`date \u0026#39;+%Y-%m-%d-%H-%M-%S\u0026#39;` training_data=$bucket$prefix/train eval_data=$bucket$prefix/validation train_source={S3DataSource={S3DataType=S3Prefix,S3DataDistributionType=ShardedByS3Key,S3Uri=$training_data}} eval_source={S3DataSource={S3DataType=S3Prefix,S3DataDistributionType=FullyReplicated,S3Uri=$eval_data}} aws --region $region \\ sagemaker create-training-job \\ --role-arn $arn_role \\ --training-job-name $training_job_name \\ --algorithm-specification TrainingImage=$training_image,TrainingInputMode=File \\ --resource-config InstanceCount=5,InstanceType=ml.c4.2xlarge,VolumeSizeInGB=10 \\ --input-data-config ChannelName=train,DataSource=$train_source,CompressionType=None,RecordWrapperType=None ChannelName=validation,DataSource=$eval_source,CompressionType=None,RecordWrapperType=None \\ --output-data-config S3OutputPath=$bucket$prefix \\ --hyper-parameters feature_dim=25,mini_batch_size=500,predictor_type=regressor,epochs=2,num_models=32,loss=absolute_loss \\ --stopping-condition MaxRuntimeInSeconds=1800 Save your file, then in your terminal window, run the following command to start your second training job now, there is no need to wait for the first training job to complete: ``` ./sharded.sh ``` In the SageMaker console, click Jobs in the left panel to check the status of the training jobs, which run concurrently. When they are complete, their Status column will change from InProgress to Complete. As a reminder, duration of these jobs can last up to about 10 minutes, including time for setting up the training cluster, as shown in the Duration column of the Jobs table. \u0026gt; To check the actual training time (not including cluster setup) for each job when both are complete, click the training job name in the jobs table, then examine the **Training duration** listed at the top right under **Job Settings**. **Training duration** does not include the time related to cluster setup. As you can see, and might expect, the sharded distribution type trained substantially faster than the fully replicated type. This is a key differentiator to consider when preparing data and picking the distribution type. Model Creation Now that you\u0026rsquo;ve trained your machine learning models, you\u0026rsquo;ll want to make predictions by setting up a hosted endpoint for them. The first step in doing that is to create a SageMaker model object that wraps the actual model artifact from training. To create the model object, you will point to the model.tar.gz that came from training and the inference code container, then create the hosting model object. You\u0026rsquo;ll do this twice, once for each model you trained earlier. Here are the steps to do this via the SageMaker console (see screenshot below for an example of all relevant fields filled in for the Oregon AWS Region): In the left pane of the SageMaker console home page, right click the Models link and open it in another tab of your browser. Click the Create Model button at the upper right above the \u0026lsquo;Models\u0026rsquo; table.\nFor the \u0026lsquo;Model name\u0026rsquo; field under Model Settings, enter distributed-replicated.\nFor the \u0026lsquo;Location of inference code image\u0026rsquo; field under Primary Container, enter the name of the same Docker image you specified previously for the region where you\u0026rsquo;re running this workshop. For ease of reference, here are the image names again:\nN. Virginia: 382416733822.dkr.ecr.us-east-1.amazonaws.com/linear-learner:latest Oregon: 174872318107.dkr.ecr.us-west-2.amazonaws.com/linear-learner:latest Ohio: 404615174143.dkr.ecr.us-east-2.amazonaws.com/linear-learner:latest Ireland: 438346466558.dkr.ecr.eu-west-1.amazonaws.com/linear-learner:latest For the \u0026lsquo;Location of model artifacts\u0026rsquo; field under Primary Container, enter the path to the output of your replicated training job. To find the path, go back to your first browser tab, click Jobs in the left pane, then find and click the replicated job name, which will look like linear-replicated-\u0026lt;date\u0026gt;. Scroll down to the Outputs section, then copy the path under \u0026lsquo;S3 model artifact\u0026rsquo;. Paste the path in the field; it should look like s3://sagemaker-projects-pdx/sagemaker/data_distribution_types/linear-replicated-2018-03-11-18-13-13/output/model.tar.gz.\nClick Create model at the bottom of the page.\nRepeat the above steps for the sharded training job model, except: for \u0026lsquo;Model name\u0026rsquo;, enter distributed-sharded, and for \u0026lsquo;Location of model artifacts\u0026rsquo;, enter the path for the sharded training job model artifact.\nModel Serving and Evaluation Once you\u0026rsquo;ve setup your models, you can configure what your hosting endpoints should be. Here you specify the EC2 instance type to use for hosting, the initial number of instances, and the hosting model name. Again, you\u0026rsquo;ll do this twice, once for each model we trained earlier. Here are the steps to do this via the SageMaker console (see screenshot below for an example of all relevant fields filled in for the Oregon AWS Region): In the left pane of the SageMaker console, click Endpoint configuration. Click the Create endpoint configuration button at the upper right above the \u0026lsquo;Endpoint configuration\u0026rsquo; table.\nFor the \u0026lsquo;Endpoint configuration name\u0026rsquo; field under New endpoint configuration, enter distributed-replicated.\nUnder Production variants, click Add model. From the Add model popup, select the distributed-replicated model you created earlier, and click Save. Then click Create endpoint configuration at the bottom of the page.\nRepeat the above steps for the sharded training job model, except: for \u0026lsquo;Endpoint configuration name\u0026rsquo;, enter distributed-sharded, and for the Add model popup, select the distributed-sharded model.\nFor the final step in the process of setting up endpoints, you\u0026rsquo;ll use the SageMaker console to do so (see screenshot below for an example of all relevant fields filled in for the Oregon AWS Region): In the left pane of the SageMaker console, click Endpoints. Click the Create endpoint button at the upper right above the \u0026lsquo;Endpoints\u0026rsquo; table.\nFor the \u0026lsquo;Endpoint name\u0026rsquo; field under Endpoint, enter distributed-replicated.\nUnder Attach endpoint configuration, leave \u0026lsquo;Use an existing endpoint configuration\u0026rsquo; selected, then under Endpoint configuration, select distributed-replicated from the table, then click Select endpoint configuration at the bottom of the table. Then click Create endpoint at the bottom of the page.\nRepeat the above steps, except: for \u0026lsquo;Endpoint name\u0026rsquo;, enter distributed-sharded, and for the Endpoint configuration table, select the distributed-sharded endpoint configuration.\nIn the Endpoints table, refer to the \u0026lsquo;Status\u0026rsquo; column, and wait for both endpoints to change from \u0026lsquo;Creating\u0026rsquo; to \u0026lsquo;InService\u0026rsquo; before proceeding to the next step. It will take several minutes for endpoint creation, possibly as long as ten minutes.\nTo compare predictions from your two models, let\u0026rsquo;s return to the notebook you used earlier (jump to the \u0026lsquo;Evaluate\u0026rsquo; section). When you are finished, return here and proceed to the next section. Conclusion \u0026amp; Extensions In this module, you ran a regression on a relatively artificial example, and skipped some pre-processing steps along the way (like potentially transforming or winsorizing our target variable, looking for interations in our features, etc.). But the main point was to highlight the difference in training time and accuracy of a model trained through two different distribution methods.\nOverall, sharding data into separate files and sending them to separate training nodes will run faster, but may produce lower accuracy than a model that replicates the data across all nodes. Naturally, this can be influenced by training the sharded model longer, with more epochs. And it should be noted that we trained with a very small number of epochs to highlight this difference.\nDifferent algorithms can be expected to show variation in which distribution mechanism is most effective at achieving optimal compute spend per point of model accuracy. The message remains the same though, that the process of finding the right distribution type is another experiment in optimizing model training times.\n"
},
{
	"uri": "https://sagemaker-workshop.com/cleanup/workspace.html",
	"title": "CloudFormation Stacks",
	"tags": [],
	"description": "",
	"content": "To delete all the underlying resources, assume the Account Admin Role and navigate to CloudFormation.\nIf the \u0026ldquo;view nested\u0026rdquo; button is checked, click on it, so you only see the root stacks. You should see a root stack associated with the Notebook, one associated with the DS Environment and a secure-ds-core stack which builds the core environment with the Shared Services VPC. Delete this one last.\nStep by Step Instructions\nFirst ensure that your S3 buckets associated with this workshop are deleted. They should contain your Project Name.\nNext, delete the most recent stack starting with \u0026ldquo;SC-\u0026lt;account_num\u0026gt;-pp-####\u0026rdquo;. Be Careful: Make sure to delete the root stack, nested stacks have a tab that says \u0026ldquo;nested\u0026rdquo; on top of the stack. These are deleted automatically when the root stack is deleted, and don\u0026rsquo;t need to be separately deleted.\nThen delete the second stack starting with \u0026ldquo;SC-\u0026lt;account_num\u0026gt;-pp-####\u0026rdquo;. This stack is created by the Data Science Admin to provision the secure environment for data scientists. Once again do not delete nested stacks separately.\nFinally, delete the secure-ds-core stack. Note: nested stacks will be automatically deleted by the root stack.\nNote, that if you choose to build this environment in a different region, you may get an error that the Data Science Adminstrator IAM roles already exists. Delete that role first before deploying to another region.\n"
},
{
	"uri": "https://sagemaker-workshop.com/custom/containers.html",
	"title": "Overview of containers for Amazon SageMaker",
	"tags": [],
	"description": "",
	"content": "SageMaker makes extensive use of Docker containers to allow users to train and deploy algorithms. Containers allow developers and data scientists to package software into standardized units that run consistently on any platform that supports Docker. Containerization packages code, runtime, system tools, system libraries and settings all in the same place, isolating it from its surroundings, and insuring a consistent runtime regardless of where it is being run.\nWhen you develop a model in Amazon SageMaker, you can provide separate Docker images for the training code and the inference code, or you can combine them into a single Docker image.\nAnatomy of an Amazon SageMaker container Using this powerful container environment, developers can deploy any kind of code in the Amazon SageMaker ecosystem. You can also create a logical division of labor by creating a deployment team, such as a DevOps, security, and infrastructure teams (who maintains the container) and a data scientists team (who focuses on producing models that are later added to the container).\nExample container folder The Bring Your Own scikit Algorithm example provides a detailed walkthrough on how to package a scikit-learn algorithm for training and production-ready hosting using containers. Let\u0026rsquo;s take a look at the container folder structure to explain how Amazon SageMaker runs Docker for training and hosting your own algorithm.\ncontainer/ Dockerfile build_and_push.sh decision_trees/ nginx.conf predictor.py serve train wsgi.py Let\u0026rsquo;s discuss each of these in turn:\nDockerfile describes how to build your Docker container image. More details below. build_and_push.sh is a script that uses the Dockerfile to build your container images and then pushes it to Amazon ECR. decision_trees is the directory which contains the files that will be installed in the container. In this simple application, only five files are installed in the container. You may only need that many or, if you have many supporting routines, you may wish to install more. These five show the standard structure of our Python containers, although you are free to choose a different toolset and therefore could have a different layout. If you\u0026rsquo;re writing in a different programming language, you\u0026rsquo;ll certainly have a different layout depending on the frameworks and tools you choose.\nThe files that will be installed in the container are:\nnginx.conf is the configuration file for the nginx front-end. predictor.py is the program that actually implements the Flask web server and the decision tree predictions for this app. serve is the program started when the container is started for hosting. It simply launches the gunicorn server which runs multiple instances of the Flask app defined in predictor.py. train is the program that is invoked when the container is run for training. wsgi.py is a small wrapper used to invoke the Flask app. The Dockerfile The Dockerfile describes the image that you want to build. You can think of it as describing the complete operating system installation of the system that you want to run. A Docker container running is quite a bit lighter than a full operating system, however, because it takes advantage of Linux on the host machine for the basic operations.\nFor the Python science stack, we will start from a standard Ubuntu installation and run the normal tools to install the things needed by scikit-learn. Finally, we add the code that implements our specific algorithm to the container and set up the right environment to run under.\nAlong the way, we clean up extra space. This makes the container smaller and faster to start.\nLet\u0026rsquo;s look at the Dockerfile for the example:\n# Build an image that can do training and inference in SageMaker # This is a Python 2 image that uses the nginx, gunicorn, flask stack # for serving inferences in a stable way. FROM ubuntu:16.04 MAINTAINER Amazon AI \u0026lt;sage-learner@amazon.com\u0026gt; RUN apt-get -y update \u0026amp;\u0026amp; apt-get install -y --no-install-recommends \\ wget \\ python \\ nginx \\ ca-certificates \\ \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* # Here we get all python packages. # There\u0026#39;s substantial overlap between scipy and numpy that we eliminate by # linking them together. Likewise, pip leaves the install caches populated which uses # a significant amount of space. These optimizations save a fair amount of space in the # image, which reduces start up time. RUN wget https://bootstrap.pypa.io/get-pip.py \u0026amp;\u0026amp; python get-pip.py \u0026amp;\u0026amp; \\ pip install numpy scipy scikit-learn pandas flask gevent gunicorn \u0026amp;\u0026amp; \\ (cd /usr/local/lib/python2.7/dist-packages/scipy/.libs; rm *; ln ../../numpy/.libs/* .) \u0026amp;\u0026amp; \\ rm -rf /root/.cache # Set some environment variables. PYTHONUNBUFFERED keeps Python from buffering our standard # output stream, which means that logs can be delivered to the user quickly. PYTHONDONTWRITEBYTECODE # keeps Python from writing the .pyc files which are unnecessary in this case. We also update # PATH so that the train and serve programs are found when the container is invoked. ENV PYTHONUNBUFFERED=TRUE ENV PYTHONDONTWRITEBYTECODE=TRUE ENV PATH=\u0026#34;/opt/program:${PATH}\u0026#34; # Set up the program in the image COPY decision_trees /opt/program WORKDIR /opt/program Running a container for Amazon SageMaker training Amazon SageMaker invokes the training code by running a version of the following command:\ndocker run \u0026lt;image\u0026gt; train This means that your Docker image should have an executable file in it that is called train. You will modify this program to implement your training algorithm. This can be in any language that is capable of running inside of the Docker environment, but the most common language options for data scientists include Python, R, Scala, and Java. For our Scikit example, we use Python.\nAt runtime, Amazon SageMaker injects the training data from an Amazon S3 location into the container. The training program ideally should produce a model artifact. The artifact is written, inside of the container, then packaged into a compressed tar archive and pushed to an Amazon S3 location by Amazon SageMaker.\nWhen Amazon SageMaker runs training, your train script is run just like any regular program. A number of files are laid out for your use, under the /opt/ml directory:\n/opt/ml  input   config    hyperparameters.json    resourceConfig.json   data   \u0026lt;channel_name\u0026gt;   \u0026lt;input data\u0026gt;  model   \u0026lt;model files\u0026gt;  output  failure The input /opt/ml/input/config contains information to control how your program runs. hyperparameters.json is a JSON-formatted dictionary of hyperparameter names to values. These values will always be strings, so you may need to convert them. resourceConfig.json is a JSON-formatted file that describes the network layout used for distributed training. Since scikit-learn doesn\u0026rsquo;t support distributed training, we\u0026rsquo;ll ignore it here. /opt/ml/input/data/\u0026lt;channel_name\u0026gt;/ (for File mode) contains the input data for that channel. The channels are created based on the call to CreateTrainingJob but it\u0026rsquo;s generally important that channels match what the algorithm expects. The files for each channel will be copied from S3 to this directory, preserving the tree structure indicated by the S3 key structure. /opt/ml/input/data/\u0026lt;channel_name\u0026gt;_\u0026lt;epoch_number\u0026gt; (for Pipe mode) is the pipe for a given epoch. Epochs start at zero and go up by one each time you read them. There is no limit to the number of epochs that you can run, but you must close each pipe before reading the next epoch. The output /opt/ml/model/ is the directory where you write the model that your algorithm generates. Your model can be in any format that you want. It can be a single file or a whole directory tree. SageMaker will package any files in this directory into a compressed tar archive file. This file will be available at the S3 location returned in the DescribeTrainingJob result. /opt/ml/output is a directory where the algorithm can write a file failure that describes why the job failed. The contents of this file will be returned in the FailureReason field of the DescribeTrainingJob result. For jobs that succeed, there is no reason to write this file as it will be ignored. Running a container for SageMaker hosting Amazon SageMaker invokes hosting service by running a version of the following command\ndocker run \u0026lt;image\u0026gt; serve This launches a RESTful API to serve HTTP requests for inference. Again, this can be done in any language or framework that works within the Docker environment.\nIn most Amazon SageMaker containers, serve is simply a wrapper that starts the inference server. Furthermore, Amazon SageMaker injects the model artifact produced in training into the container and unarchives it automatically.\nAmazon SageMaker uses two URLs in the container:\n/ping will receive GET requests from the infrastructure. Your program returns 200 if the container is up and accepting requests. /invocations is the endpoint that receives client inference POST requests. The format of the request and the response is up to the algorithm. If the client supplied ContentType and Accept headers, these will be passed in as well. Storing SageMaker Containers For SageMaker to run a container for training or hosting, it needs to be able to find the image hosted in the image repository, Amazon Elastic Container Registry (Amazon ECR). The three main steps to this process are building locally, tagging with the repository location, and pushing the image to the repository.\nTo build the local image, call the following command:\ndocker build \u0026lt;image name\u0026gt; This takes instructions from the Dockerfile we discussed earlier to generate an image on your local instance. After the image is built, we need to let our local instance of Docker know where to store the image so that SageMaker can find it. We do this by tagging the image with the following command:\ndocker tag \u0026lt;image name\u0026gt; \u0026lt;repository name\u0026gt; The repository name has the following structure:\n\u0026lt;account number\u0026gt;.dkr.ecr.\u0026lt;region\u0026gt;.amazonaws.com/\u0026lt;image name\u0026gt;:\u0026lt;tag\u0026gt; Without tagging the image with the repository name, Docker defaults to uploading to Docker Hub, and not Amazon ECR. Amazon SageMaker currently requires Docker images to reside in Amazon ECR. To push an image to ECR, and not the central Docker registry, you must tag it with the registry hostname.\nUnlike Docker Hub, Amazon ECR images are private by default, which is a good practice with Amazon SageMaker. If you want to share your Amazon SageMaker images publicly, you can find more information in the Amazon ECR User Guide.\nFinally, to upload the image to Amazon ECR, with the Region set in the repository name tag, call the following command:\ndocker push \u0026lt;repository name\u0026gt; One final note on Amazon SageMaker Docker containers. We have already shown you that you have the option to build one Docker container serving both training and hosting, or you can build one for each. While building two Docker images can increase storage requirements and cost due to duplicated common libraries, you might get a benefit from building a significantly smaller inference container, allowing the hosting service to scale more quickly when reacting to traffic increases. This is especially common when you use GPUs for training, but your inference code is optimized for CPUs. You need to consider the tradeoffs when you decide if you want to build a single container or two.\n"
},
{
	"uri": "https://sagemaker-workshop.com/security_for_sysops/security_overview.html",
	"title": "Security Overview",
	"tags": [],
	"description": "",
	"content": "Amazon SageMaker is a powerful enabler and a key component of a data science environment, but it\u0026rsquo;s only part of what is required to build a complete secure data science environment. For more robust security you will need other AWS services such as Amazon CloudWatch, Amazon S3, and AWS VPC. To begin, let\u0026rsquo;s talk about some of the key things you will want in place, working in concert, with Amazon SageMaker.\nPrivate Network Environment Let\u0026rsquo;s begin with your Virtual Private Cloud (VPC) which will be used to host Amazon SageMaker and other components of your data science environment. An AWS VPC provides a familiar set of network-level controls to allow you to govern ingress and egress of data. You will begin this workshop by creating a shared services VPC with no Internet Gateway (IGW), therefore all subnets will be private, without Internet connectivity. Network connectivity with AWS services or to your shared services from other VPCs will be provided using VPC endpoints powered by PrivateLink. Security Groups will be used to control traffic between different resources, allowing you to group like resources together and manage their ingress and egress traffic.\nIn these labs you will create a VPC that:\nHas no IGW or NAT gateway attached Multiple subnets are defined across Availability Zones for resiliency VPC Endpoints are configured to grant explicit access to AWS services like Amazon SageMaker APIs, Amazon S3, STS, and CloudWatch Logs Security Groups are configured to govern IP traffic and grant access to VPC endpoints DNS Hostnames are enabled on the VPC to support VPC endpoint hostname resolution Authentication and Authorization AWS Identity and Access Management (IAM) can help you create preventive controls for many aspects of your data science enviroment. They can control access to your data in Amazon S3, control who can access SageMaker resources like Notebook servers, and even be applied as VPC endpoint policies to put explicit controls around the API endpoints you create in your data science environment.\nThere are several IAM roles you will need in order to manage permissions and ensure separation of concerns at scale. Those roles are:\nData scientist user role\nGranting Console access, permissions to start/stop a Jupyter notebook, permissions to open a Jupyter notebook\nNotebook creation role\nUsed by CI/CD pipeline or Service Catalog to create a Jupyter Notebook\nNotebook execution role\nUsed by a Jupyter Notebook to access AWS resources\nTraining / Transform job execution role\nUsed by Training Jobs or Batch Transform jobs to access AWS resources like Amazon S3\nEndpoint creation role\nUsed by your CI/CD pipeline to create hosted ML model endpoints\nEndpoint hosting role\nUsed by a hosted ML model to access AWS resources such as Amazon S3\nEndpoint invocation role\nUsed by an application to call a hosted ML model endpoint\nThere are also many IAM conditions you can apply in your policies to begin to grant powerful permissions but only under certain conditions. You will learn more about these in the labs.\nData Protection In a data science environment there is the highly sensitive data you are using to train your ML models, but there is also the sensitive intellectual property you are developing in the form of algorithms, libraries, and trained models. There are many ways to protect data such as the preventive controls described above, defined as IAM policies. In addition you have the ability to encrypt data at rest using managed encryption keys.\nMany AWS services, including Amazon S3 and Amazon SageMaker, are integrated with AWS Key Management Service (KMS) to make it very easy to encrypt your data at rest. You can take advantage of these integrations to ensure that your data is encrypted in the data lake AND in the data science environment, end to end. This encryption also applies to your intellectual property as it is being developed in the many places it may be stored such as Amazon S3, EC2 EBS volumes, or AWS CodeCommit git repository.\nAuditability Using cloud services in a safe and responsible manner is good, but being able to demonstrate to others that you are operating in a governed manner is even better. Developers and security officers alike will need to see activity logs for models being trained and persons interacting with the systems. Amazon CloudWatch Logs and CloudTrail are there to help, receiving logs from many different parts of your data science environment to include:\nAmazon S3 Amazon SageMaker Notebooks Amazon SageMaker Training Jobs Amazon SageMaker Hosted Models VPC Flow Logs Let\u0026rsquo;s now dive into implementing these key areas. You will start by creating a secure, private, network environment to host shared services along with a self-service mechanism to support project administrators.\n"
},
{
	"uri": "https://sagemaker-workshop.com/security_for_users/security_overview.html",
	"title": "Security Overview",
	"tags": [],
	"description": "",
	"content": "Amazon SageMaker is a powerful enabler and a key component of a data science environment, but it\u0026rsquo;s only part of what is required to build a complete secure data science environment. For more robust security you will need other AWS services such as Amazon CloudWatch, Amazon S3, and AWS VPC. To begin, let\u0026rsquo;s talk about some of the key things you will want in place, working in concert, with Amazon SageMaker.\nPrivate Network Environment Let\u0026rsquo;s begin with your Virtual Private Cloud (VPC) which will be used to host Amazon SageMaker and other components of your data science environment. Your VPC provides a familiar set of network-level controls to allow you to govern ingress and egress of data. We will begin this workshop by creating a VPC with no Internet Gateway (IGW), therefore all subnets will be private, without Internet connectivity. Network connectivity with AWS services or your own shared services will be provided using VPC endpoints and PrivateLink. Security Groups will be used to control traffic between different resources, allowing you to group like resources together and manage their ingress and egress traffic.\nIn these labs you will create a VPC that:\nHas no IGW or NAT gateway attached Multiple subnets are defined across Availability Zones for resiliency VPC Endpoints are configured to grant explicit access to Amazon SageMaker APIs, Amazon S3, STS, and CloudWatch Logs Security Groups are configured to govern IP traffic and grant access to VPC endpoints DNS Hostnames enabled on the VPC to support VPC endpoint hostname resolution Authentication and Authorization AWS Identity and Access Management (IAM) can help you create preventive controls for many aspects of your data science enviroment. They can control access to your data in Amazon S3, control who can access SageMaker resources like Notebook servers, and even be applied as VPC endpoint policies to put explicit controls around the API endpoints you create in your data science environment.\nThere are several IAM roles you will need in order to manage permissions and ensure separation of concerns at scale. Those roles are:\nData scientist user role\nGranting Console access, start/stop Jupyter notebook, open Jupyter notebook\nNotebook creation role\nUsed by CI/CD pipeline or Service Catalog to create a Jupyter Notebook\nNotebook execution role\nUsed by a Jupyter Notebook to access AWS resources\nTraining / Transform job execution role\nUsed by Training Job or Batch Transform job to access AWS resources like Amazon S3\nEndpoint creation role\nUsed by your CI/CD pipeline to create hosted ML model endpoints\nEndpoint hosting role\nUsed by a hosted ML model to access AWS resources such as Amazon S3\nEndpoint invocation role\nUsed by an application to call a hosted ML model endpoint\nThere are also many IAM conditions you can apply in your policies to begin to grant powerful permissions but only under certain conditions. You will learn more about these in the labs.\nData Protection In a data science environment there is the highly sensitive data you are using to train your ML models, but there is also the sensitive intellectual property you are developing in the form of algorithms, libraries, and trained models. There are many ways to protect data such as the preventive controls described above, defined as IAM policies. In addition you have the ability to encrypt data at rest using managed encryption keys.\nMany AWS services, including Amazon S3 and Amazon SageMaker, are integrated with AWS Key Management Service (KMS) to make it very easy to encrypt your data at rest. You can take advantage of these integrations to ensure that your data is encrypted in the data lake AND in the data science environment, end to end. This encryption also applies to your intellectual property as it is being developed in the many places it may be stored such as Amazon S3, EC2 EBS volumes, or AWS CodeCommit git repository.\nAuditability Using cloud services in a safe and responsible manner is good, but being able to demonstrate to others that you are operating in a governed manner is even better. Developers and security officers alike will need to see activity logs for models being trained and persons interacting with the systems. Amazon CloudWatch Logs and CloudTrail are there to help, receiving logs from many different parts of your data science environment to include:\nAmazon S3 Amazon SageMaker Notebooks Amazon SageMaker Training Jobs Amazon SageMaker Hosted Models VPC Flow Logs Let\u0026rsquo;s now dive into implementing these key areas. You will start by creating a secure, private, network environment.\n"
},
{
	"uri": "https://sagemaker-workshop.com/prerequisites/prerequisites.html",
	"title": "Environment",
	"tags": [],
	"description": "",
	"content": "AWS Account In order to complete this workshop you\u0026rsquo;ll need an AWS Account, and an AWS IAM user in that account with at least full permissions to the following AWS services:\nAWS IAM Amazon S3 Amazon SageMaker AWS Cloud9 Use Your Own Account: The code and instructions in this workshop assume only one student is using a given AWS account at a time. If you try sharing an account with another student, you\u0026rsquo;ll run into naming conflicts for certain resources. You can work around these by appending a unique suffix to the resources that fail to create due to conflicts, but the instructions do not provide details on the changes required to make this work. Use a personal account or create a new AWS account for this workshop rather than using an organizations account to ensure you have full access to the necessary services and to ensure you do not leave behind any resources from the workshop.\nCosts: Some, but NOT all, of the resources you will launch as part of this workshop are eligible for the AWS free tier if your account is less than 12 months old. See the AWS Free Tier page for more details. An example of a resource that is not covered by the free tier is the ml.m4.xlarge notebook instance used in some workshops. To avoid charges for endpoints and other resources you might not need after you\u0026rsquo;ve finished a workshop, please refer to the Cleanup Module.\nAWS Region SageMaker is not available in all AWS Regions at this time. Accordingly, we recommend running this workshop in one of the following supported AWS Regions: N. Virginia, Oregon, Ohio, or Ireland.\nOnce you\u0026rsquo;ve chosen a region, you should create all of the resources for this workshop there, including a new Amazon S3 bucket and a new SageMaker notebook instance. Make sure you select your region from the dropdown in the upper right corner of the AWS Console before getting started.\nBrowser We recommend you use the latest version of Chrome or Firefox to complete this workshop.\nAWS Command Line Interface To complete certain workshop modules, you\u0026rsquo;ll need the AWS Command Line Interface (CLI) and a Bash environment. You\u0026rsquo;ll use the AWS CLI to interface with SageMaker and other AWS services.\nFor these workshops, AWS Cloud9 is used to avoid problems that can arise configuring the CLI on your machine. AWS Cloud9 is a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code with just a browser. It has the AWS CLI pre-installed so you dont need to install files or configure your laptop to use the AWS CLI. For Cloud9 setup directions for these workshops, see Cloud9 Setup. Do NOT attempt to use a locally installed AWS CLI during a live workshop because there is insufficient time during a live workshop to resolve related issues.\nText Editor For any workshop module that requires use of the AWS Command Line Interface (see above), you also will need a plain text editor for writing Bash scripts. Any editor that inserts Windows or other special characters potentially will cause scripts to fail. AWS Cloud9 includes a text editor.\n"
},
{
	"uri": "https://sagemaker-workshop.com/introduction/concepts.html",
	"title": "Machine Learning with Amazon SageMaker",
	"tags": [],
	"description": "",
	"content": "This section describes a typical machine learning workflow and summarizes how you accomplish those tasks with Amazon SageMaker.\nIn machine learning, you \u0026ldquo;teach\u0026rdquo; a computer to make predictions, or inferences. First, you use an algorithm and example data to train a model. Then you integrate your model into your application to generate inferences in real time and at scale. In a production environment, a model typically learns from millions of example data items and produces inferences in hundreds to less than 20 milliseconds.\nThe following diagram illustrates the typical workflow for creating a machine learning model:\nAs the diagram illustrates, you typically perform the following activities:\nGenerate example data To train a model, you need example data. The type of data that you need depends on the business problem that you want the model to solve (the inferences that you want the model to generate). For example, suppose you want to create a model to predict a number given an input image of a handwritten digit. To train such a model, you need example images of handwritten numbers.\nData scientists often spend a lot of time exploring and preprocessing, or \u0026ldquo;wrangling,\u0026rdquo; example data before using it for model training. To preprocess data, you typically do the following:\nFetch the data You might have in-house example data repositories, or you might use datasets that are publicly available. Typically, you pull the dataset(s) into a single repository.\nClean the dataTo improve model training, inspect the data and clean it up as needed. For example, if your data has a country name attribute with values United States and US, you might want to edit the data to be consistent.\nPrepare or transform the dataYou might perform additional data transformations to improve performance. For example, you might choose to combine attributes. If your model predicts the conditions that require de-icing an aircraft, instead of using temperature and humidity attributes separately, you might combine them into a new attribute to get a better model.\nIn Amazon SageMaker, you preprocess example data in a Jupyter notebook on your notebook instance. You use your notebook to fetch your dataset, explore it and prepare it for model training. You\u0026rsquo;ll create a Notebook Instance in the next section.\nTrain a model Model training includes both training and evaluating the model, as follows:\nTraining the modelTo train a model, you need an algorithm. The algorithm you choose depends on a number of factors. In Amazon SageMaker, you have the following options for a training algorithm:\nUse an algorithm provided by Amazon SageMakerAmazon SageMaker provides training algorithms. If one of these meets your needs, it\u0026rsquo;s a great out-of-the-box solution for quick model training. For a list of algorithms provided by Amazon SageMaker, see the Amazon SageMaker documentation. You\u0026rsquo;ll use some of the built-in SageMaker algorithms in the Using Built-in Algorithms module.\nUse Apache Spark with Amazon SageMakerAmazon SageMaker provides a library that you can use in Apache Spark to train models with Amazon SageMaker. Using the library provided by Amazon SageMaker is similar to using Apache Spark MLLib. For more information, see Using Apache Spark with Amazon SageMaker.\nSubmit custom code to train with deep learning frameworksYou can submit custom Python code that uses TensorFlow or Apache MXNet for model training. You\u0026rsquo;ll see an example of using Apache MXNet with Amazon SageMaker in the Using Custom Algorithms module.\nUse your own custom algorithmsPut your code together as a Docker image and specify the registry path of the image in an Amazon SageMaker CreateTrainingJob API call. For more information, see the Amazon SageMaker documentation. You\u0026rsquo;ll see an example of how to use your own algorithm in the Using Custom Algorithms module.\nEvaluating the modelAfter you\u0026rsquo;ve trained your model, you evaluate it to determine whether the accuracy of the inferences is acceptable. You can evaluate your model using historical data (offline) or live data:\nOffline testingUse historical, not live, data to send requests to the model for inferences. Deploy your trained model to an alpha endpoint, and use historical data to send inference requests to it. To send the requests, use a Jupyter notebook in your Amazon SageMaker notebook instance and either the AWS SDK for Python (Boto) or the high-level Python library provided by Amazon SageMaker.\nOnline testing with live dataAmazon SageMaker supports multiple models (called production variants) to a single Amazon SageMaker endpoint. You configure the production variants so that a small portion of the live traffic goes to the model that you want to validate. For example, you might choose to send 10% of the traffic to a model variant for evaluation. After you are satisfied with the model\u0026rsquo;s performance, you can route 100% traffic to the updated model.\nYou use a Jupyter notebook in your Amazon SageMaker notebook instance to train and evaluate your model.\nDeploy the model Traditionally, you do some re-engineering of a model to integrate it with your application, before deploying the model into production. With Amazon SageMaker hosting services, you can deploy your model independently, decoupling it from your application code. For more information, see Deploying a Model on Amazon SageMaker Hosting Services.\nMachine learning is a continuous cycle. After deploying a model, you monitor the inferences, then collect \u0026ldquo;ground truth,\u0026rdquo; and evaluate the model to identify drift. You then increase the accuracy of your inferences by updating your training data to include the newly collected ground truth, by retraining the model with the new dataset. As more and more example data becomes available, you continue retraining your model to increase accuracy over time.\n"
},
{
	"uri": "https://sagemaker-workshop.com/prerequisites.html",
	"title": "Prerequisites",
	"tags": [],
	"description": "",
	"content": "In this module we\u0026rsquo;ll go through the prerequisites for the workshop, and setup a Cloud9 workspace for the workshop.\n"
},
{
	"uri": "https://sagemaker-workshop.com/security_for_sysops/team_resources/secure_networking.html",
	"title": "Secure Networking",
	"tags": [],
	"description": "",
	"content": "Amazon SageMaker allows you to create resources attached to your AWS Virtual Private Cloud (VPC). This allows you to govern access to SageMaker resources and your data sets using familiar tools such as security groups, routing tables, and VPC endpoints. Using these network-layer tools you can create a secure network environment that allows you to explicitly control the data ingress and egress of your data science environment. Please take a few moments and read about these tools in more detail.\nPrivate Network Environment Begin with your VPC which will be used to host Amazon SageMaker and other components of your data science environment. Your VPC provides a familiar set of network-level controls to allow you to govern ingress and egress of data. You will begin this lab by creating a VPC with no Internet Gateway (IGW), therefore all subnets will be private, without Internet connectivity. Network connectivity with AWS services or your own shared services will be provided using VPC endpoints powered by PrivateLink. Security Groups will be used to control traffic between different resources, allowing you to group like resources together and manage their ingress and egress traffic.\nVirtual Private Cloud A Virtual Private Cloud (VPC) gives you a self-contained network environment that you control. When initially created the VPC does not allow network traffic into or out of the VPC. It\u0026rsquo;s only by adding VPC endpoints, Internet Gateways (IGW), or Virtual Private Gateways (VPGW) that you begin to configure your private network environment to communicate with the wider world. For the rest of these labs you can assume that no access to the internet is required and that all communication with AWS services will be done explicitly through private connectivity to the AWS APIs through VPC endpoints. It is also recommend to create multiple subnets in your VPC in multiple availability zones to support highly available deployments and resilient architectures.\nTo find out more about VPCs and VPC concepts such as routing tables, subnets, security groups, and network access control lists please visit the AWS documentation.\nVPC Endpoints A VPC endpoint allows you to establish a private, secure connection between your VPC and an AWS service without requiring you to configure an Internet Gateway, NAT device, or VPN connection. Using VPC endpoints your VPC resources can communicate with AWS services without ever leaving the AWS network. A VPC endpoint is a highly available virtual device that is managed on your behalf. As a VPC resource an endpoint is given IP addresses within your VPC and security groups assigned to the endpoint can control who can communicate with the endpoint.\nIn addition to security groups you can also apply VPC endpoint policies to an AWS service endpoint. An endpoint policy is an IAM resource policy that gets applied to a VPC endpoint and governs which APIs can be called on an AWS service through the endpoint. For example, if the following endpoint policy were applied to an Amazon S3 VPC endpoint it would only allow read access to objects in the specified S3 bucket. Actions against other buckets would be denied.\n{ \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Access-to-specific-bucket-only\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34; ], \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::my_secure_bucket\u0026#34;, \u0026#34;arn:aws:s3:::my_secure_bucket/*\u0026#34; ] } ] } VPC endpoint policies, along with security groups, provide the ability to implement Defense-in-Depth and bring a multi-layered approach to security and who can access what resources within your VPC.\nIn this lab you will create a VPC with endpoints for the following services:\nAmazon S3, for reading and writing data Amazon SageMaker, for creating training jobs and hosted models Amazon STS, for obtaining temporary credentials Amazon CloudWatch Logs, for writing out log data from VPC-based resources Use the previously created AWS Service Catalog products to create an AWS Virtual Private Cloud, security groups, and VPC endpoints to configure a precise, secure network environment to support your data science project teams. The VPC will have no IGW or NAT gateway attached and it will have multiple subnets across 3 availability zones. VPC endpoints will be created and attached to the VPC and security groups applied to control what VPC resources can communicate with each other.\n"
},
{
	"uri": "https://sagemaker-workshop.com/security_for_users/environment/secure_notebook.html",
	"title": "Secure Notebooks",
	"tags": [],
	"description": "",
	"content": "Amazon SageMaker is designed to empower data scientists and developers, enabling them to build more quickly and remain focused on their machine learning project. One of the ways that SageMaker does this is by providing hosted Jupyter Notebook servers. With a single API call users can create a Jupyter Notebook server with the latest patches and kernels available. No need to install software, patch or maintain systems - it is all done for you. What the API creates for you is an EC2 instance running Jupyter Notebook server software with Python, R, Docker, and the most popular ML frameworks pre-installed. But it is not just about convenience and enablement, SageMaker is also focused on hosting these notebooks for you in a secure manner.\nSecure by default An Amazon SageMaker notebook is an EC2 instance with the open source Jupyter server installed. The SageMaker service manages the EC2 instance in order to help you maintain a level of security with little or no overhead. To do this Amazon SageMaker takes responsibility for:\nPatching and updating the system Encrypting data on the EBS volumes Limiting user access to the system Enabling you to further tailor and harden the system Patching and updating A SageMaker Jupyter notebook server will have 2 EBS volumes attached to it. The first is the system drive and is ephemeral. This hosts the operating system, Anaconda, and Jupyter server software. The second hosts your data and anything you put into /home/ec2-user/SageMaker. Over time your EC2 instance can become out of date, going unpatched. To patch your Jupyter Notebook to the latest versions simply stop the notebook and start it again. This will refresh the system drive without any maintenance required on your part. However please note that if you make any changes to files on the system drive you will need to make those changes again as they will be destroyed in the stopping and starting of the notebook server.\nTo keep a SageMaker notebook up to date and to save costs it is recommended to stop a Jupyter notebook server when it is not needed and to restart it when you need to use it.\nEncryption at rest As mentioned the EC2 instance has two EBS volumes mapped to it. Both of these volumes are encrypted using a SageMaker service-managed KMS key, although you can specify a CMK to be used to encrypt the storage volume. By doing this you can easily encrypt all of the data stored on the Jupyter Notebook server by default.\nLimiting user access The very nature of software development means that users can obtain some OS-level access to the Jupyter notebook server. By default the Jupyter Web UI will allow you to open a shell terminal. When a user access the shell, they will be logged into the EC2 instance as ec2-user. If you\u0026rsquo;re familiar with Amazon Linux this is the default username that you use to gain access to an AWS EC2 instance. This user also typically has permissions to sudo to the root user. This can be limited in the Jupyter Notebook configuration which will remove the user\u0026rsquo;s permission to assume the root user. They will still have permissions to install things like Python modules into their user environment, but they will not be able to modify the wider system of the notebook server.\nFurther tailoring In addition to the measures described above you also have the ability to specify Lifecycle Configurations. These are shell scripts which execute on system bootstrap, before the notebook is made available to users. These configuration scripts can execute on system creation, system startup, or both. Using these scripts you can ensure that monitoring agents are installed or other types of hardening are performed to ensure that the system is in a specific state before allowing users to access the system. Here we will use the lifecycle scripts to download some open source libraries from the pip mirror we created, create an sagemaker_environment.py file to keep track of variables such as the network configuration, KMS keys that can be imported directly, without giving the datascientist access to them.\nManaged and Governed Amazon SageMaker provides managed EC2-based services such as Jupyter notebooks, training jobs, and hosted ML models. SageMaker runs the infrastructure for these components using EC2 resources dedicated to yourself. These EC2 resources can be mapped to your VPC environment allowing you to apply your network level controls, such as security groups, to the notebooks, training jobs, and hosted ML models.\nAmazon SageMaker does this by creating an ENI in your specified VPC and attaching it to the EC2 infrastructure in the service account. Using this pattern the service gives you control over the network-level access of the services you run on Amazon SageMaker.\nFor Jupyter Notebooks this will mean that all Internet connectivity, Amazon S3 connectivity, or access to other systems in your VPC is governed by you and by your network configuration.\nFor training jobs and hosted models these are again governed by you. When retrieving training data from Amazon S3 the SageMaker EC2 instances will communicate with S3 through your VPC without traversing the public internet. Equally when the SageMaker EC2 instances retrieve your trained model for hosting, they will communicate with S3 through your VPC, without traversing the public internet. You maintain governance and control of the network communication of your SageMaker resources.\nAccess Control Access to a SageMaker Jupyter notebook instance is goverend by AWS IAM. In order to open a Jupyter Notebook instance users will need access to the CreatePresignedNotebookInstanceUrl API call. This API call creates a presigned URL which can be followed to obtain Web UI access to the Jupyter notebook server. To secure this interface you use IAM policy statements to wrap conditions around calling the API, for example who can invoke the API and from what IP address.\n{ \u0026#34;Id\u0026#34;:\u0026#34;notebook-example-1\u0026#34;, \u0026#34;Version\u0026#34;:\u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;:[ { \u0026#34;Sid\u0026#34;:\u0026#34;Enable Notebook Access\u0026#34;, \u0026#34;Effect\u0026#34;:\u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;:[ \u0026#34;sagemaker:CreatePresignedNotebookInstanceUrl\u0026#34;, \u0026#34;sagemaker:DescribeNotebookInstance\u0026#34; ], \u0026#34;Resource\u0026#34;:\u0026#34;*\u0026#34;, \u0026#34;Condition\u0026#34;:{ \u0026#34;ForAnyValue:StringEquals\u0026#34;:{ \u0026#34;aws:sourceVpce\u0026#34;:[ \u0026#34;vpce-111bbccc\u0026#34;, \u0026#34;vpce-111bbddd\u0026#34; ] } } } ] } The IAM policy above states that someone can only communicate with a Notebook if they do so from within a VPC and through specific VPC endpoints. Using mechanisms like the above you can explicitly control who can interact with a Notebook server. Other example IAM policies can be found here: https://docs.aws.amazon.com/sagemaker/latest/dg/security_iam_id-based-policy-examples.html\nAs a best practice, we want to limit a single user to a notebook instance. As every notebook instance has an associated IAM role, by using fine grained user level IAM roles, we can limit a single user to a single notebook instance. Similarly, for cost attribution purposes we can use tagging https://docs.aws.amazon.com/IAM/latest/UserGuide/id_tags.html to link usage costs to individual users or to teams.\nVersion Control Finally, to support collaboration, SageMaker notebooks can be integrated with Git-based repositories. Git is a distributed version control system which enables project teams to manage source code, notebook files, and other artifacts related to a project. In the next lab the notebook you create will be configured to use a CodeCommit repository as a way of managing the sample project provided. In this lab you will push and tag the code to the master branch of the code repository using the CLI or Jupyter UI.\nIn this lab, as a data scientist, you will use the newly launched notebook to kickoff a new ML project.\n"
},
{
	"uri": "https://sagemaker-workshop.com/security_for_users/notebook/ml_lifecycle.html",
	"title": "Security Objectives",
	"tags": [],
	"description": "",
	"content": "A secure environment is an enabler for teams, allowing them to focus on their project and the business challenge it is trying to solve. From the perspective of a project team the environment is intended to support the team in achieving the following objectives:\nCompute and Network Isolation\nIn working with the Amazon SageMaker service you will need to configure training jobs and similar resources using practices inline with security policy. This means provisioning resources in the same secure network environment in which your JupyterLab server is operating. This lab will show you how to configure SageMaker to provision resources in accordance with your policies.\nAuthentication and Access Management\nAn AWS IAM role has been provisioned for you and for the resources you create, this lab will show you how to pass that role on to Amazon SageMaker for use when acting on your behalf.\nArtifact Management\nYour team has been provided with resources and technology. Your team will now produce artifacts such as features derived from data, notebooks to explore and conduct experimentation, algorithms to produce ML models, and other forms of intellectual property. This lab will show you how to manage these assets using services like Amazon S3 and an AWS CodeCommit Git repository.\nData Encryption\nYou will learn how to ensure that all training/processing volumes are passed appropriate encryption keys to ensure data is encrypted end to end, in transit and at rest. Also we will see how to use network configurations to ensure that data is encrypted in transit and doesn\u0026rsquo;t traverse the internet.\nTraceability and Auditability\nYou will learn how to use SageMaker Experiments to easily maintain the lineage of your trained models and training jobs. SageMaker Experiments will automatically manage the metadata associated with your training jobs and make it easily accessible via a Pandas DataFrame.\nExplainability and Interpretability\nThere are different methods, tools, and techniques to explain an ML model\u0026rsquo;s inference. In this lab you will use SHAP values to derive feature importances and gain insight into how your model is weighting different inputs to produce a prediction.\nReal Time Model Monitoring\nOnce a model has been produced ensuring that it is producing predictions that are still accurate and relevant for the current state of the world is a must to avoid drift in original accuracy, F1 scores, or other objective metrics. In this lab you will learn how to monitor your production endpoints in real time to detect concept drift and alert when inputs to your model or its outputs are no longer in keeping with the model\u0026rsquo;s training sets.\nReproducibility\nFor audit reasons, you may need to reproduce your work at a later time. To ensure reproducibility, you will see how to use SageMaker experiments in combination with CodeCommit to version your code and track the entire lineage of your models.\nIn the following labs you will work through Jupyter notebooks which show you how to support the above objectives using the environment that has been provisioned for you.\nIn the first lab you will work through a notebook called 01_SageMaker-DataScientist-Workflow.ipynb which will cover a typical Data Scientist workflow and show you how to explore data, pre-process data, train an XGBoost model using an Amazon Managed container and explore feature importances for that model in a secure manner, maintaining network traffic within your team\u0026rsquo;s private VPC and enforing encryption at rest and in transit. Furthermore, you will learn how to use SageMaker Processing for running processing jobs at scale, and leverage Spot instance pricing to save on training costs.\nIn the second notebook, 02_SageMaker-DevOps-Workflow.ipynb, you will deploy the trained model from the SageMaker notebook to production and monitor the endpoint for data drift using ModelMonitor. Finally you will use SageMaker Experiments to track any model metadata, such as the Git commit ID associated with the algorithm, to trace the lineage of our models and endpoints.\n"
},
{
	"uri": "https://sagemaker-workshop.com/security_for_sysops/team_resources/secure_environment_lab.html",
	"title": "Lab 2: Secure Environment",
	"tags": [],
	"description": "",
	"content": "A data science project team have requested a cloud environment to begin their project. As a project administrator you will use the Service Catalog portfolio, managed by the Cloud Platform Engineering team, to provision a secure VPC and related resources for the data science team. In this lab, you will use AWS Service Catalog to provision this data science environment. By following the steps below you will create an environment which contains:\nAWS VPC with no IGW VPC endpoints to Amazon S3, Amazon SageMaker, CloudWatch, STS IAM roles for the project\u0026rsquo;s data science administrator and the data scientists Store AWS resource names/identifiers in AWS SSM Parameter Store for future reference Service Catalog Portfolio of products for the data science team AWS KMS key for encrypting data at rest Amazon S3 buckets dedicated to the project Amazon CodeCommit Git repository for project source code Deploy a project environment In the previous lab you deployed a CloudFormation template which created a project administrator role. Details of this role can be found in the CloudFormation Outputs for the stack you deployed.\nAfter assuming the project administrator role, visit the AWS Service Catalog console and provision a project environment for the data science team.\nEnvironment creation will take approximately 10 minutes.\nStep-by-step instructions\rOpen the AWS CloudFormation console and select the stack you deployed in the previous lab. Click the Outputs tab on the stack\u0026rsquo;s detail page and notice the AssumeProjectAdminRole hyperlink to assume the project administrator role created by the stack. Click the hyperlink to assume the project administrator role. On the resulting screen leave the values unchanged and click Switch Role. As the project administrator:\nVisit the AWS Service Catalog console. Click the context menu button next to the product Data Science Project Environment and click Launch product. Give the product deployment a name, such as example-project-dev-environment and click Next. Provide a ProjectName such as example-project. You can leave the remaining values unchanged if you like. Click Next through the next few screens to get to the Review page. Click Launch. If you wish to see the contents of these CloudFormation templates you can view them on the CloudFormation console or copy them locally for review using a command such as the below.\naws s3 sync s3://sagemaker-workshop-cloudformation-us-east-1/quickstart ./sagemaker-workshop-cloudformation Review base environment After the CloudFormation stack has been successfully created review the Resources tab of the CloudFormation stack and the resources that were created. You\u0026rsquo;ll notice that it has provisioned:\nProject-specific Service Catalog Portfolio\nA service catalog portfolio and products have been configured to give the data science team a tailored set of products they can deploy easily.\nIAM roles\nRoles and permissions have been created so that the data science teams can manage themselves and create the resources they need.\nSSM parameters\nA collection of parameters have been stored so they can be referenced by the data science teams. Visit the console, what parameters have been created?\nKMS key\nA KMS key to encrypt data at rest in the data science environment. Visit the console, what is the KMS key being used to encrypt?\nVirtual Private Cloud (VPC)\nThe template has created a VPC with no Internet connectivity but with VPC endpoints for accessing AWS services like Amazon S3 and Amazon SageMaker. Visit the console, what services are accessible from within the VPC? Do any endpoints have Endpoint Policies governing them?\nYou have now created a secure environment for the data science team. Now hand things over to the project team and let them support themselves.\n"
},
{
	"uri": "https://sagemaker-workshop.com/introduction.html",
	"title": "Introduction to Amazon SageMaker",
	"tags": [],
	"description": "",
	"content": "This module demonstrates the main features of SageMaker via a set of straightforward examples for common use cases. You\u0026rsquo;ll go through some Machine Learning concepts and how they relate to Amazon SageMaker as well as create a SageMaker Notebook Instance for the workshop.\n"
},
{
	"uri": "https://sagemaker-workshop.com/prerequisites/jupyter.html",
	"title": "Jupyter Notebooks",
	"tags": [],
	"description": "",
	"content": "Jupyter is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text. Uses include: data cleaning and transformation, numerical simulation, statistical modelling, data visualization, machine learning, and much more. With respect to code, it can be thought of as a web-based IDE that executes code on the server it is running on instead of locally.\nThere are two main types of \u0026ldquo;cells\u0026rdquo; in a notebook: code cells, and \u0026ldquo;markdown\u0026rdquo; cells with explanatory text. You will be running the code cells. These are distinguished by having \u0026ldquo;In\u0026rdquo; next to them in the left margin next to the cell, and a greyish background. Markdown cells lack \u0026ldquo;In\u0026rdquo; and have a white background. In the screenshot below, the upper cell is a markdown cell, while the lower cell is a code cell:\nTo run a code cell, simply click in it, then either click the Run Cell button in the notebook\u0026rsquo;s toolbar, or use Control+Enter from your computer\u0026rsquo;s keyboard. It may take a few seconds to a few minutes for a code cell to run. You can determine whether a cell is running by examining the In[]: indicator in the left margin next to each cell: a cell will show In [*]: when running, and In [a number]: when complete.\nPlease run each code cell in order, and only once, to avoid repeated operations. For example, running the same training job cell twice might create two training jobs, possibly exceeding your service limits.\n"
},
{
	"uri": "https://sagemaker-workshop.com/security_for_users/environment/env_deploy_lab.html",
	"title": "Lab 01: Deploy the environment",
	"tags": [],
	"description": "",
	"content": "In the following steps you will use AWS CloudFormation and AWS Service Catalog to create a self-service mechanism to create secure data science environments. You will first deploy a CloudFormation template which provisions a shared service environment which hosts a PyPI mirror along with a detective control to enforce Amazon SageMaker resources being attached to a VPC. The template will also create a product portfolio in AWS Service Catalog which enables users with appropriate permissions to create a data science environment dedicated to a single project. Once this environment is created you will move on to the next lab which will use AWS Service Catalog to provision a SageMaker notebook.\nUsing one of the buttons below deploy the Secure Data Science quickstart into your AWS account. This will provision a shared services VPC which hosts a PyPI mirror. The CloudFormation stack will also create an AWS Service Catalog to allow project administrators to create data science environments using the self-service mechanisms of the AWS Service Catalog.\nAfter the CloudFormation stack has been deployed visit the Outputs tab of the CloudFormation console. You will notice a hyperlink which will allow you to the assume the role of a Project Administrator. Click this link and then navigate to the Service Catalog console and launch a Data Science Project Environment.\nRegion Launch Template Oregon (us-west-2) Deploy to AWS Oregon\rOhio (us-east-2) Deploy to AWS Ohio\rN. Virginia (us-east-1) Deploy to AWS N. Virginia\rIreland (eu-west-1) Deploy to AWS Ireland\rLondon (eu-west-2) Deploy to AWS London\rSydney (ap-southeast-2) Deploy to AWS Sydney\rStep-by-Step instructions\rVisit the CloudFormation console and select the Outputs tab for the stack you just deployed. Use the provided link to assume the role of Project Administrator. Navigate to the Service Catalog console and launch a Data Science Project Environment. Click the context menu button in the upper-right of the Data Science Project Environment tile and select Launch product. Give the provisioned product a name, such as project-abc-environment and click Next. Use a ProjectName such as project-abc and click Next. Note S3 bucket names need to be globally unique, so don\u0026rsquo;t use project-abc verbatim but replace \u0026ldquo;abc\u0026rdquo; with something unique such as \u0026ldquo;yourname-12345\u0026rdquo; etc. Click Next without entering any Tag Options. Click Next without checking any Notifications. On the Review page click Launch. The product will take approximately 10 minutes to launch.\nAt this point, the cloud platform engineering team has created the underlying infrastructure to host secure data science environments. The project administrators have provisioned a data science environment for your project team and they have provided you with an AWS Service Catalog to enable you to provision SageMaker notebooks. In the next lab you will use these resources to provision a Jupyter notebook and start developing your ML solution.\n"
},
{
	"uri": "https://sagemaker-workshop.com/security_for_users/notebook/notebook_lab_01.html",
	"title": "Lab 03: Data Science Workflow",
	"tags": [],
	"description": "",
	"content": "The ML lifecylce has many stages and steps, and often requires revisiting previous steps as you tune your model. This lab is intended to highlight how your project team can work through the ML lifecycle and achive the objectives outlined earlier through supporting services such as experiment tracking.\nThe notebook explained 01_SageMaker-DataScientist-Workflow.ipynb will cover a typical Data Scientist workflow of data exploration, model training, extracting model feature importances and committing your code to Git.\nYou will look at a credit card dataset to predict whether a customer will default on their credit card payments based on prior payment data. This is a binary classification problem and you will train a XGBoost model using an Amazon managed container and explore feature importances for that model.\nThroughout this notebook you will keep network traffic within your private VPC and enforce encryption at rest and in transit. Furthermore, you will learn how to use SageMaker Processing for running processing jobs at scale, and leverage Spot instance pricing to save up to 90% on training costs.\nFinally you will commit the code to our CodeCommit repository and demonstrate code versioning, tagging and source control.\nAfter you have completed the notebook please proceed to Lab 04: DevOps Workflow where you will deploy and monitor the trained model.\n"
},
{
	"uri": "https://sagemaker-workshop.com/security_for_sysops/best_practice/service_catalog.html",
	"title": "Self-Service with Guard Rails",
	"tags": [],
	"description": "",
	"content": "Allowing users in the cloud to self-service and provision cloud resources on-demand is a powerful enabler for project teams but can be a concern from an operational risk perspective. However, if you can empower your developers to self-service, while enforcing guard rails and best practice, then the operational and security teams will also benefit. With enforced guard rails and best practice you can be confident that, while developers are creating resources they need, they are doing so in a manner that is inline with your policies and requirements.\nAWS Service Catalog AWS Service Catalog allows you to create and manage collections of logical IT products and services that you have configured and parameterized as templates. These IT template products and services can include everything from virtual machines, software deployment, and database creation to complete multi-tier application architectures. AWS Service Catalog allows you to centrally manage commonly deployed IT services, and helps you achieve consistent governance and meet your compliance requirements, while enabling users to quickly deploy only the approved IT services they need.\nAWS Service Catalog works by managing CloudFormation templates you provide, allowing users in your AWS environment to deploy the templates. You control the templates themselves along with who can deploy or manage the templates and the resulting deployments. With AWS Service Catalog you can control which IT services and versions are available, the configuration of the available services, and permission access by individual, group, department, or cost center.\nTo get started you begin by creating a portfolio which represents a collection of products and configuration information. It allows you to specify who can access the products you define and how they can use them.\nOnce you\u0026rsquo;ve created a portfolio you then begin to define products which describe a product or service that your users can deploy onto AWS. A product will consist of one or more AWS resources like an Amazon EC2 instance, databases, or Jupyter notebooks. These products are described through a CloudFormation template and can be tailored by the user at deployment time based upon the values of parameters you define for the user to configure.\nWith resources deployed on the user\u0026rsquo;s behalf you will then want to ensure that users can access those resources in an appropriate manner without being able to undo the best practice configurations deployed by Service Catalog. To do this you can implement preventive controls in the form of identity and access management policies.\nIdentity \u0026amp; Accesss Management AWS services are interacted with via a RESTful API. Every call into this API is authorized by the AWS Identity \u0026amp; Access Managment (IAM) service. You control AWS IAM and grant users in your environment explicit permissions to use various AWS services. You grant explicit permissions through IAM policy documents which specify the principal (who), the actions (API calls), and the resources (such as S3 objects) that are allowed, as well as under what conditions such access is granted.\n{ \u0026#34;Version\u0026#34;:\u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;:[ { \u0026#34;Effect\u0026#34;:\u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;:[ \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:DeleteObjectVersion\u0026#34; ], \u0026#34;Resource\u0026#34;:\u0026#34;arn:aws:s3:::my_corporate_bucket/home/${aws:userid}/*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;IpAddress\u0026#34;: { \u0026#34;aws:SourceIp\u0026#34;: \u0026#34;10.1.100.0/24\u0026#34; } } } ] } The example policy document above grants a user permissions to read or write to an S3 bucket, but only to a sub-folder that matches the user\u0026rsquo;s user ID, and only from an IP address on the 10.1.100.0 network. We can see that the Effect explicitly allows the Actions on a single Resource.\nAmazon SageMaker has a comprehensive set of conditions and actions which you can grant to users in your environment. You can grant users the ability to start, stop, or access notebook servers, create training jobs, or host models in production for example. You can also use an array of conditions such as:\nsagemaker:VolumeKmsKey, to require that an encryption key is specified sagemaker:VpcSecurityGroupIds, to require that a VPC configuration is provided sagemaker:DirectInternetAccess, to ensure that notebooks do not have Internet access sagemaker:NetworkIsolation, to ensure that models or training jobs cannot communicate with the network In this lab you will create an IAM role for the project administrators. You will also create a Service Catalog portfolio and product, granting project administrators the ability to deploy data science environments using this templated product in Service Catalog. You will also create a detective control (covered later) to detect any Amazon SageMaker resources that are run in a non-compliant manner. Lastly you will create a PyPI mirror to provide private access to approved Python packages from the data science environments.\n"
},
{
	"uri": "https://sagemaker-workshop.com/security_for_sysops/best_practice/best_practice_lab.html",
	"title": "Lab 1: Best Practice as Code",
	"tags": [],
	"description": "",
	"content": "Before you can begin creating templates for deployment by the Project Administration team you will need a shared services VPC to host a Python package mirror (PyPI) for use by data science teams. The mirror will host a collection of approved Python packages. The concept of a shared services VPC or PyPI mirror is not something that is detailed in this workshop, and is partially assumed as common practice among many AWS customers. After you have created a shared services VPC and PyPI mirror you will then, as the Cloud Platform Engineering Team, create a Service Catalog Portfolio which the project administrators can use to easily deploy data science environments in support of new projects.\nThis lab assumes other recommended security practices such as enabling AWS CloudTrail and capturing VPC Flow Logs. The contents of this lab focus soley on controls and guard rails directly related to data science resources.\nShared Services architecture In this section you will quickly get started by deploying a shared PyPI mirror for use by data science project teams. In addition to deploying a shared service this template will also create an IAM role for use by the AWS Service Catalog and for use by project administrators who are responsible for creating data science project environments.\nThe shared PyPI mirror will be hosted in a shared services VPC and exposed to project environments using a PrivateLink-powered endpoint. The mirror will host approved Python packages and can be used by all internal Python applications, such as machine learning code running on SageMaker.\nThe resulting architecture will look like this:\nDeploy your shared service As a cloud platform engineering team member, deploy the CloudFormation template linked below to provision a shared service VPC and IAM roles.\nRegion Launch Template Oregon (us-west-2) Deploy to AWS Oregon\rOhio (us-east-2) Deploy to AWS Ohio\rN. Virginia (us-east-1) Deploy to AWS N. Virginia\rIreland (eu-west-1) Deploy to AWS Ireland\rLondon (eu-west-2) Deploy to AWS London\rSydney (ap-southeast-2) Deploy to AWS Sydney\rDeployment should take around 5 minutes.\nStep-by-Step instructions\rClick the button above which is appropriate for the AWS region in which you want to build. This will take you to the CloudFormation console to create a CloudFormation stack using the reference CloudFormation template. Check the Stack name and the name you want to use for your shared service resources. You can accept the default values if you wish. Under Capabilities click the 2 check boxes indicating you understand that the CloudFormation template will create IAM resources. Click Create stack Create Project Portfolio With the shared services VPC online and available you now need to provide the project administration team with a configured Service Catalog to provision data science project environments. To start, visit the AWS Service Catalog console and create a Portfolio. Grant the DataScienceAdmin role permissions to access the portfolio adn then use the appropriate CloudFormation template linked below to create a Data Science Environment product. Ensure that the product has a constraint applied to it that uses the IAM role ServiceCatalogLaunchRole to launch the product upon request. This will give the Service Catalog service the permissions needed to create a Data Science Environment.\nService Catalog Product Templates by Region:\nRegion ap-southeast-2, https://s3.amazonaws.com/sagemaker-workshop-cloudformation-ap-southeast-2/quickstart/ds_environment.yaml Region eu-west-1, https://s3.amazonaws.com/sagemaker-workshop-cloudformation-eu-west-1/quickstart/ds_environment.yaml Region eu-west-2, https://s3.amazonaws.com/sagemaker-workshop-cloudformation-eu-west-2/quickstart/ds_environment.yaml Region us-east-1, https://s3.amazonaws.com/sagemaker-workshop-cloudformation-us-east-1/quickstart/ds_environment.yaml Region us-east-2, https://s3.amazonaws.com/sagemaker-workshop-cloudformation-us-east-2/quickstart/ds_environment.yaml Region us-west-2, https://s3.amazonaws.com/sagemaker-workshop-cloudformation-us-west-2/quickstart/ds_environment.yaml Step-by-Step instructions\rAccess the AWS Service Catalog console Click Portfolios on the left Click Create portfolio Enter a Portfolio name of Data Science Project Portfolio Enter a Owner of Cloud Operations Team Click the link for your new portfolio to view the portfolio\u0026rsquo;s details Click the Groups, roles, and users tab Click Add groups, roles, users Click Roles and type DataScienceAdmin into the search field Tick the box next to your DataScienceAdministrator role and click Add access Click Products Click Upload new product Enter a Product name of Data Science Environment For Owner enter Cloud Operations Team Click Use a CloudFormation template For the CloudFormation template URL enter the appropriate URL from the list below: For Version title enter v1 Click Review and Create product Click the radio button next to the new product and from the Actions drop down select Add product to portfolio Click the radio button for your product portfolio and click Add Product to Portfolio Return to the list of Portfolios by clicking Portfolios on the left Click the link for the data science portfolio Click the Constraints tab in the portfolio detail page Click Create constraint From the Product drop down select your product Select Launch for the Constraint type Under Launch Constraint click Select IAM role From the IAM role drop down select ServiceCatalogLaunchRole Click Create Review team resources In addition to the Service Catalog Portfolio and product you have also created the following AWS resources to support the project administration team. Please take a moment and review these resources and their configuration.\nIAM roles\nAWS IAM roles\nThe IAM roles for the project administration team and the Service Catalog have been created. Visit the AWS IAM console and review the permissions granted to these two roles.\nAWS Lambda Detective Control\nAn AWS Lambda has been deployed and configured to execute whenever an Amazon SageMaker resource is deployed. The Lambda function will act as a detective control, inspecting launched resources to ensure that the resource is configured correctly. To inspect the Lambda function and its triggers visit the AWS Lambda console. Can you determine exactly what types of events will cause the Lambda function to execute?\nParameters added to Parameter Store\nA collection of parameters have been added to Parameter Store. Can you see what parameters have been added? How would you use these values?\nShared Services VPC\nThe template has created a VPC that will house our shared applications. Visit the console and see what services are accessible from within the VPC?\nPyPI Mirror Service\nA Python package mirror has been deployed as a containerised service in the Shared Services VPC. This service is running on a cluster managed by Amazon Elastic Container Service (ECS) Fargate which means there are no Amazon EC2 servers for you to manage. Visit the ECS console to check whether the service is up and running. You can also see the task logs from the container through the ECS console to check its status.\nWith these resources created you can now move on to Lab 2 where you will, as a project administrator, deploy a secure data science environment for a new project team.\n"
},
{
	"uri": "https://sagemaker-workshop.com/prerequisites/cloud9.html",
	"title": "Cloud9 Setup",
	"tags": [],
	"description": "",
	"content": "AWS Cloud9 is a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code with just a browser. It includes a code editor, debugger, and terminal. Cloud9 comes pre-packaged with essential tools for popular programming languages and the AWS Command Line Interface (CLI) pre-installed so you dont need to install files or configure your laptop for this workshop. Your Cloud9 environment will have access to the same AWS resources as the user with which you logged into the AWS Management Console.\nSetup the Cloud9 Development Environment Go to the AWS Management Console, click Services then select Cloud9 under Developer Tools.\nClick Create environment.\nEnter workshop into Name and optionally provide a Description.\nClick Next step.\nYou may leave Environment settings at their defaults of launching a new t2.micro EC2 instance which will be paused after 30 minutes of inactivity.\nClick Next step.\nReview the environment settings and click Create environment. It will take several minutes for your environment to be provisioned and prepared.\nOnce ready, your IDE will open to a welcome screen. The central panel of the IDE has two parts: a text/code editor in the upper half, and a terminal window in the lower half. Below the welcome screen in the editor, you should see a terminal prompt similar to the following (you may need to scroll down below the welcome screen to see it):\nYou can run AWS CLI commands in here just like you would on your local computer. Verify that your user is logged in by running aws sts get-caller-identity as follows at the terminal prompt: aws sts get-caller-identity Youll see output indicating your account and user information: Admin:~/environment $ aws sts get-caller-identity { \u0026#34;Account\u0026#34;: \u0026#34;123456789012\u0026#34;, \u0026#34;UserId\u0026#34;: \u0026#34;AKIAI44QH8DHBEXAMPLE\u0026#34;, \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:iam::123456789012:user/Alice\u0026#34; } To create a new text/code file, just click the + symbol in the tabs section of the editor part of the IDE. You can do that now, and close the wecome screen by clicking the x symbol in the welcome screen tab.\nKeep your AWS Cloud9 IDE opened in a browser tab throughout this workshop as well use it for activities like using the AWS CLI and running Bash scripts.\nTips Keep an open scratch pad in Cloud9 or a text editor on your local computer for notes. When the step-by-step directions tell you to note something such as an ID or Amazon Resource Name (ARN), copy and paste that into the scratch pad.\n"
},
{
	"uri": "https://sagemaker-workshop.com/security_for_users/notebook/notebook_lab_02.html",
	"title": "Lab 04: DevOps Workflow",
	"tags": [],
	"description": "",
	"content": "In notebook 02_SageMaker-DevOps-Workflow.ipynb, you will complete the machine learning lifecycle and deliver a model into production. As a DevOps Engineer you will pick up the model trained by the data scientists and deploy it as an endpoint. You will also set up model monitoring on the endpoint for detecting data drift. This notebook is primarily focused on two aspects of running machine learning models in production, monitoring the model\u0026rsquo;s performance and being able to demonstrate the model\u0026rsquo;s lineage.\nModel Monitoring When a model is operating in production issues can arise that can be difficult to detect. For instance invalid data inputs can be sent to your model for inference, if those invalid data inputs have an invalid data type that can be easily detected, but if the value is a numeric value that is greater than any value seen previously in training, how would you detect that? Another issue could be what\u0026rsquo;s called concept drift, meaning that the patterns the model was trained to detect are no longer relevant for the world in which the model is operating. For instance, if a model was trained to detect fraud but the patterns of fraud have shifted in response to increased detection, the model would no longer be as accurate as it once was. Amazon SageMaker offers a monitoring capability to detect erroneous inputs and concept drift. In this notebook you will use Model Monitor to capture the inputs and outputs of your model and to provide reports on its observations of your model.\nAmazon SageMaker Model Monitor can detect the following violations:\nInvalid input or output data types Detection of uncharacteristic levels of null values Data distribution of input or output data is outside of baseline data distribution Missing or extra input values Unknown categorical values For more information about the types of violations and their meanings please see the Model Monitor documentation.\nNote that the Model Monitor is scheduled to execute periodically. As a result you may have to give the monitor up to an hour to execute itself as you work through the notebook. Below you will find example screen captures of monitor output, similar to what your monitoring job will show after time.\nThese graphics are generated using SageMaker Studio UI.\nThe image below shows a sample Pandas dataframe showing some of the violations which Model Monitor can detect. Note, these may not be the exact final violations you see. For example, below we have also modified the Marriage column.\nThis same dataframe is shown below as a graph. You can see that there is a large drift in the MARRIAGE column, but over time the service also detects some drift in the Label distribution. SageMaker Studio can also directly plot the actual drift as shown in the BILL_AMT figure instead of comparing the baseline to the observed values.\nBy setting thresholds for acceptable drift, you can decide when to retrain your models.\nModel Lineage As a best practice, you will commit the notebook to a git repo that is already provisioned, and call SageMaker Experiments APIs to track any model metadata, container artifacts, data and code lineage from the raw code to the production endpoint.\nAs you near the end of the notebook you will reach Part 8: Lineage. Here you will generate a dataframe that contains the lineage of your trained model, tagging the experiment with relevant Git commit ids and details.\nCongratulations, you have now taken raw data and an algorithm to produce a trained model for your business challenge. You have deployed this model and monitored it for drift, with the ability to demonstrate the lineage of the model in terms of when it was trained, which version of the code was used, etc.\n"
},
{
	"uri": "https://sagemaker-workshop.com/security_for_sysops/secure_notebook/secure_notebook.html",
	"title": "Secure Notebooks",
	"tags": [],
	"description": "",
	"content": "Amazon SageMaker is designed to empower data scientists and developers, enabling them to build more quickly and remain focused on their machine learning project. One of the ways that SageMaker does this is by providing hosted Jupyter Notebook servers. With a single API call users can create a Jupyter Notebook server with the latest patches and kernels available. No need to install software, patch or maintain systems - it is all done for you. What the API creates for you is an EC2 instance running Jupyter Notebook server software with Python, R, Docker, and the most popular ML frameworks pre-installed. But it is not just about convenience and enablement, SageMaker is also focused on hosting these notebooks for you in a secure manner.\nSecure by default An Amazon SageMaker notebook is an EC2 instance with the open source Jupyter server installed. The SageMaker service manages the EC2 instance in order to help you maintain a level of security with little or no overhead. To do this Amazon SageMaker takes responsibility for:\nPatching and updating the system Encrypting data on the EBS volumes Limiting user access to the system Enabling you to further tailor and harden the system Patching and updating A SageMaker Jupyter notebook server will have 2 EBS volumes attached to it. The first is the system drive and is ephemeral. This hosts the operating system, Anaconda, and Jupyter server software. The second hosts your data and anything you put into /home/ec2-user/SageMaker. Over time your EC2 instance can become out of date, going unpatched. To patch your Jupyter Notebook to the latest versions simply stop the notebook and start it again. This will refresh the system drive without any maintenance required on your part. However please note that if you make any changes to files on the system drive you will need to make those changes again as they will be destroyed in the stopping and starting of the notebook server.\nTo keep a SageMaker notebook up to date and to save costs it is recommended to stop a Jupyter notebook server when it is not needed and to restart it when you need to use it.\nEncryption at rest As mentioned the EC2 instance has two EBS volumes mapped to it. Both of these volumes are encrypted using a SageMaker service-managed KMS key, although you can specify a CMK to be used to encrypt the storage volume. By doing this you can easily encrypt all of the data stored on the Jupyter Notebook server by default.\nLimiting user access The very nature of software development means that users can obtain some OS-level access to the Jupyter notebook server. By default the Jupyter Web UI will allow you to open a shell terminal. When a user access the shell, they will be logged into the EC2 instance as ec2-user. If you\u0026rsquo;re familiar with Amazon Linux this is the default username that you use to gain access to an AWS EC2 instance. This user also typically has permissions to sudo to the root user. This can be limited in the Jupyter Notebook configuration which will remove the user\u0026rsquo;s permission to assume the root user. They will still have permissions to install things like Python modules into their user environment, but they will not be able to modify the wider system of the notebook server.\nFurther tailoring In addition to the measures described above you also have the ability to specify Lifecycle Configurations. These are shell scripts which execute on system bootstrap, before the notebook is made available to users. These configuration scripts can execute on system creation, system startup, or both. Using these scripts you can ensure that monitoring agents are installed or other types of hardening are performed to ensure that the system is in a specific state before allowing users to access the system. Here we will use the lifecycle scripts to download some open source libraries from the pip mirror we created, create an sagemaker_environment.py file to keep track of variables such as the network configuration, KMS keys that can be imported directly, without giving the datascientist access to them.\nManaged and Governed Amazon SageMaker provides managed EC2-based services such as Jupyter notebooks, training jobs, and hosted ML models. SageMaker runs the infrastructure for these components using EC2 resources dedicated to yourself. These EC2 resources can be mapped to your VPC environment allowing you to apply your network level controls, such as security groups, to the notebooks, training jobs, and hosted ML models.\nAmazon SageMaker does this by creating an ENI in your specified VPC and attaching it to the EC2 infrastructure in the service account. Using this pattern the service gives you control over the network-level access of the services you run on Amazon SageMaker.\nFor Jupyter Notebooks this will mean that all Internet connectivity, Amazon S3 connectivity, or access to other systems in your VPC is governed by you and by your network configuration.\nFor training jobs and hosted models these are again governed by you. When retrieving training data from Amazon S3 the SageMaker EC2 instances will communicate with S3 through your VPC. Equally when the SageMaker EC2 instances retrieve your trained model for hosting, they will communicate with S3 through your VPC. You maintain governance and control of the network communication of your SageMaker resources.\nAccess Control Access to a SageMaker Jupyter notebook instance is goverend by AWS IAM. In order to open a Jupyter Notebook instance users will need access to the CreatePresignedNotebookInstanceUrl API call. This API call creates a presigned URL which can be followed to obtain Web UI access to the Jupyter notebook server. To secure this interface you use IAM policy statements to wrap conditions around calling the API, for example who can invoke the API and from what IP address.\n{ \u0026#34;Id\u0026#34;:\u0026#34;notebook-example-1\u0026#34;, \u0026#34;Version\u0026#34;:\u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;:[ { \u0026#34;Sid\u0026#34;:\u0026#34;Enable Notebook Access\u0026#34;, \u0026#34;Effect\u0026#34;:\u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;:[ \u0026#34;sagemaker:CreatePresignedNotebookInstanceUrl\u0026#34;, \u0026#34;sagemaker:DescribeNotebookInstance\u0026#34; ], \u0026#34;Resource\u0026#34;:\u0026#34;*\u0026#34;, \u0026#34;Condition\u0026#34;:{ \u0026#34;ForAnyValue:StringEquals\u0026#34;:{ \u0026#34;aws:sourceVpce\u0026#34;:[ \u0026#34;vpce-111bbccc\u0026#34;, \u0026#34;vpce-111bbddd\u0026#34; ] } } } ] } The IAM policy above states that someone can only communicate with a Notebook if they do so from within a VPC and through specific VPC endpoints. Using mechanisms like the above you can explicitly control who can interact with a Notebook server.\nVersion Control Finally, to support collaboration, SageMaker notebooks can be integrated with Git-based repositories. Git is a distributed version control system which enables project teams to manage source code, notebook files, and other artifacts related to a project. In the next lab the notebook you create will be configured to use a CodeCommit repository as a way of managing the sample project provided. In this lab you will push and tag the code to the master branch of the code repository using the CLI or Jupyter UI.\nIn this lab, as a data scientist, you will use a product in a portfolio defined by the Project Administrators and create a Jupyter notebook for yourself following best practice.\n"
},
{
	"uri": "https://sagemaker-workshop.com/builtin.html",
	"title": "Using Built-in Algorithms",
	"tags": [],
	"description": "",
	"content": "The focus of this module is on SageMaker\u0026rsquo;s built-in algorithms. These algorithms are ready-to-use, scalable, and provide many other conveniences. The module shows how to use SageMaker\u0026rsquo;s built-in algorithms via hosted Jupyter notebooks, the AWS CLI, and the SageMaker console. You\u0026rsquo;ll also see different strategies to distribute your data when training your models across a cluster of machines. To proceed to this module you need to have completed the Cloud9 Setup and Creating a Notebook Instance sections in the previous module.\n"
},
{
	"uri": "https://sagemaker-workshop.com/security_for_users/environment/secure_notebook_lab.html",
	"title": "Lab 02: Deploy a Notebook",
	"tags": [],
	"description": "",
	"content": "Your project team has been presented with IAM roles and a Service Catalog Portfolio to allow your team to self service and obtain resources to support your efforts. Following the steps below use this portfolio to create a Jupyter notebook instance for yourself.\nLaunch the notebook product Click into the details for the data science environment product you provisioned in the last step and find the link to assume the role of a data science user. After assuming this role return to the Service Catalog console and launch an Amazon SageMaker Jupyter notebook.\nStep-by-Step instructions\rFrom the Service Catalog console open the details for the data science project environment you provisoined as a Project Administrator. Locate the link to assume the role of a Data Science User and assume the role. Now, as a Data Science User, return to the Service Catalog console and launch a SageMaker Notebook product. Click the context menu button in the upper right corner of the SageMaker Notebook product and click Launch Product. Give the product a name such as my-sagemaker-nb and click Next. Provide a NotebookOwnerEmail to be associated with any git commits. Enter the ProjectName that was used in the previous lab, such as project-abc. Enter a NotebookOwnerUsername to be associated with any git commits and click Next. Enter no Tag Options and click Next. Do not configure Notifications and click Next. On the Review page click Launch. You will land at a Provisioned Product page while the Service Catalog creates your Jupyter Notebook. Periodically click Refresh until the Status reads Succeeded. This should take about 5 minutes to launch your notebook.\nIAM Permissions Every SageMaker notebook has permissions granted to it to be able to access, create, and delete AWS resources and APIs. These permissions are granted through an IAM role associated with the Jupyter notebook\u0026rsquo;s EC2 instance. An example of the permissions associated with the notebook are highlighted in the next set of labs.\nThe IAM role assigned to your notebook has been created just for your notebook and represents an association of the Jupyter notebook to both yourself and your project. This will be represented in audit logs, identifying that actions were taken by your Jupyter notebook and allow for easy tracking of which notebook, for which project, belonging to which project team member performed an action on AWS resources.\nAccess the notebook After the notebook product has finished provisioning you can open it by clicking the NotebookUrl link provided as an Output in the provisioned product detail. With your Jupyter notebook open, familiarize yourself with the web interface and open the Notebook kernel named 01_SageMaker-DataScientist-Workflow.ipynb. Don\u0026rsquo;t forget to reference back to the Jupyter cheat sheet for a quick reference if you need one.\nStep-by-step instructions\rFrom the Service Catalog console open the link for the notebook product you provisioned. Under Outputs, click the link for NotebookUrl to launch the Jupyter notebook interface. With the notebook open, launch the Notebook kernel listed on the left hand side named 01_SageMaker-DataScientist-Workflow.ipynb. When its open the Notebook kernel should use the conda_python3 kernel. If Jupyter asks you to set the kernel select conda_python3 and if Jupyter displays an error, reload the Jupyter page by clicking your browser Refresh button.\nYou have now created a Jupyter notebook dedicated to yourself as a member of the project team. With the notebook open you are ready to take on the next set of labs where you will use the notebook and the data science environment to engineer a feature set, train a model, deploy the model, and then monitor the model for performance or drift.\n"
},
{
	"uri": "https://sagemaker-workshop.com/security_for_sysops/secure_notebook/secure_notebook_lab.html",
	"title": "Lab 3: Deploy a Jupyter Notebook",
	"tags": [],
	"description": "",
	"content": "At this point, the cloud platform engineering team has built a self-service mechanism to provision secure environments to host data science projects. The project administrators have provisioned resources using the self-service mechanism for your team to work, and they have provided you with a self-service mechanism to enable you to provision SageMaker notebooks. Now, use these resources to provision a Jupyter notebook and start developing your ML solution.\nLaunch the notebook product Navigate to the AWS Service Catalog console and on go to the detail page for the recently provisioned data science environment. Use the hyperlink labeled AssumeProjectUserRole under Outputs to assume the role of a data science user. Assume the role and visit the Service Catalog product listing. Using the notebook product defined for you by the project administrators, launch a Notebook product using the same project name that was used to create the environment.\nStep-by-step instructions\rOpen the Service Catalog provisioned products list. Click on the provisioned data science project and locate the hyperlinks in the Output section of the provisioned product detail. Click on the AssumeProjectUserRole hyperlink. Click Assume Role on the next screen. Return to the Service Catalog product listing. Open the menu for the SageMaker notebook product and click Launch Product Give the notebook product a name such as YOUR-NAME-ds-notebook and click Next Use the same project name as earlier in the labs Enter an email address for NotebookOwnerEmail and a username for NotebookOwnerUsername Click Next and on the next 2 screens click Next On the Review screen click Launch You will land at a Provisioned Product page while the Service Catalog creates your Jupyter Notebook. Periodically click Refresh until the Status reads Succeeded. This should take about 5 minutes to launch your notebook.\nAccess the notebook After the notebook has launched successfully you can open it by clicking the NotebookUrl hyperlink in the Outputs section of the provisoined notebook details page. With your Jupyter notebook open, familiarize yourself with the web interface and open the Notebook kernel named 00_SageMaker-SysOps-Workflow. Don\u0026rsquo;t forget to reference back to the Jupyter cheat sheet for a quick reference if you need one.\nWhen its open the Notebook kernel should use the conda_python3 kernel. If Jupyter asks you to set the kernel select conda_python3 and if Jupyter displays an error, reload the Jupyter page by clicking your browser Refresh button.\nYou will now learn about methods for implementing detective and corrective controls on AWS. In the next lab you will return to the Jupyter notebook to test the detective controls.\n"
},
{
	"uri": "https://sagemaker-workshop.com/introduction/notebook.html",
	"title": "Creating a Notebook Instance",
	"tags": [],
	"description": "",
	"content": "SageMaker provides hosted Jupyter notebooks that require no setup, so you can begin processing your training data sets immediately. With a few clicks in the SageMaker console, you can create a fully managed notebook instance, pre-loaded with useful libraries for machine learning. You need only add your data.\nYou\u0026rsquo;ll start by creating an Amazon S3 bucket that will be used throughout the workshop. You\u0026rsquo;ll then create a SageMaker notebook instance, which you will use for the other workshop modules.\nCreate a S3 Bucket SageMaker typically uses S3 as storage for data and model artifacts. In this step you\u0026rsquo;ll create a S3 bucket for this purpose. To begin, sign into the AWS Management Console, https://console.aws.amazon.com/.\nKeep in mind that your bucket\u0026rsquo;s name must be globally unique across all regions and customers. We recommend using a name like smworkshop-firstname-lastname. If you get an error that your bucket name already exists, try adding additional numbers or characters until you find an unused name.\nIn the AWS Management Console, choose Services then select S3 under Storage.\nChoose Create Bucket\nProvide a globally unique name for your bucket such as \u0026lsquo;smworkshop-firstname-lastname\u0026rsquo;.\nSelect the Region you\u0026rsquo;ve chosen to use for this workshop from the dropdown.\nChoose Create in the lower left of the dialog without selecting a bucket to copy settings from.\nLaunch the Notebook Instance In the upper-right corner of the AWS Management Console, confirm you are in the desired AWS region. Select N. Virginia, Oregon, Ohio, or Ireland.\nClick on Amazon SageMaker from the list of all services. This will bring you to the Amazon SageMaker console homepage.\nTo create a new notebook instance, go to Notebook instances, and click the Create notebook instance button at the top of the browser window.\nType smworkshop-[First Name]-[Last Name] into the Notebook instance name text box, and select ml.m4.xlarge for the Notebook instance type.\nFor IAM role, choose Create a new role, and in the resulting pop-up modal, select Specific S3 buckets under S3 Buckets you specify  optional. In the text field, paste the name of the S3 bucket you created above, AND the following bucket name separated from the first by a comma: gdelt-open-data. The combined field entry should look similar to smworkshop-john-smith, gdelt-open-data. Click Create role.\nYou will be taken back to the Create Notebook instance page. Click Create notebook instance.\nAccess the Notebook Instance Wait for the server status to change to InService. This will take several minutes, possibly up to ten but likely less.\nClick Open. You will now see the Jupyter homepage for your notebook instance.\n"
},
{
	"uri": "https://sagemaker-workshop.com/security_for_sysops/detective/detective_controls.html",
	"title": "Detective Controls",
	"tags": [],
	"description": "",
	"content": "AWS recommends a defense-in-depth approach to security, applying security at every level of your application and environment. In this section we will focus on the concept of detective controls and incident response in the form of a corrective control. A detective control is responsible for identifying potential security threats or incidents while a corrective control is responsible for limiting the potential damage of the threat or incident after detection.\nOne example of a form of detective control is the use of internal auditing to ensure that an environment and user practice is inline with your policies and requirements. These controls can help your organization identify and understand the scope of anomalous activity.\nMany AWS services are available to help you implement detective controls. AWS CloudTrail records AWS API calls, AWS Config provides a detailed inventory of your AWS resources and configuration. Amazon GuardDuty is a managed threat detection service that continuously monitors for malicious or unauthorized behavior. Amazon CloudWatch is a monitoring service for AWS resources which can trigger CloudWatch Events to automate security responses.\nEven with detective controls, you should still put processes in place to respond to and mitigate the potential impact of security incidents.\nAmazon CloudWatch Events allows you to create rules that trigger automated responses such as the execution of AWS Lambda.\nAmazon CloudWatch Events Amazon CloudWatch recieves metrics, logs, and events and can, in near-real time, respond to this information on your behalf. You can use log entries from applications or AWS services to automatically notify a member of your team when an error has occurred. You can use metrics to scale up or scale out your infrastructure when a CPU is exceeding a utilization threshold. Or you can inspect a launching EC2 instance that is the result of an API call into AWS.\nLogs, metrics, and events can trigger a number of responses such as emailing alerts or executing a Lambda function in response.\nA list of AWS services and the events that they send to CloudWatch can be found in the documentation. Within Amazon CloudWatch you can configure an events rule which can detect anomalous activity and trigger a response.\nThe response that is triggered can be any of a number of activities to include executing an AWS Lambda function, an Amazon ECS task, or an SSM RunCommand. This gives you flexibility to automate responses to incidents in your environment.\nThe detective and corrective controls made possible by CloudWatch Events and AWS Lambda allow you to inspect SageMaker training jobs, EC2 instance security groups, and much more. By detecting resources that are not in line with your standards you can quickly respond to and correct the resource using automated incident response.\n"
},
{
	"uri": "https://sagemaker-workshop.com/custom.html",
	"title": "Using Custom Algorithms",
	"tags": [],
	"description": "",
	"content": "This module focuses on using your own training algorithm and your own inference code. You can use the deep learning containers provided by Amazon SageMaker for model training and your own inference code. You provide a script written for the deepV learning framework, such as Apache MXNet or TensorFlow. To proceed to this module you need to have completed the Cloud9 Setup and Creating a Notebook Instance sections in the previous module.\n"
},
{
	"uri": "https://sagemaker-workshop.com/security_for_sysops/detective/detective_lab.html",
	"title": "Lab 4: Detective and Corrective Controls",
	"tags": [],
	"description": "",
	"content": "In this lab you will test a remediating detective control that was deployed by the cloud platform engineering team in Lab 1. The control is designed to detect the creation of Amazon SageMaker training jobs outside of the secure data science VPC and terminate them. To do this, you will go through the Jupyter notebook kernel 00_SageMaker-SysOps-Workflow and execute the cells up to and including the creation of a training job. You\u0026rsquo;ll notice, as the training job is provisioned and begins to execute, that it is terminated by the corrective aspect of the control in the environment.\nStart a training job As the data scientist, read through and execute the cells in the Jupyter Notebook kernel named 00_SageMaker-SysOps-Workflow.ipynb. These various cells will:\nset variables to reference the data and model Amazon S3 buckets that the administrators created for your team copy training data to your local Jupyter notebook preprocess the data using SageMaker Processing push processed training data to your data S3 bucket configure a training job to be executed by you Use SageMaker Experiments to track metadata of your training jobs When you reach the cell titled Train without a VPC configured, execute the cell and take note of the output. After a few minutes you should notice that the training job was terminated. The output should resemble the below which indicates that the training job did not complete its bootstrap.\nStep-by-step instructions\rBegin at the JupyterLabs interface of your notebook instance and execute the first cells of the notebook kernel. Continue executing the cells through Sections A and B, until you reach Section C, Part 5. When you reach the cells titled Train without a VPC configured, pause - the following cells should fail after a period when you execute them. Watch the output of the training as it executes, and notice the job does not complete its bootstrap. 2019-10-11 13:50:37 Starting - Starting the training job... 2019-10-11 13:51:01 Starting - Launching requested ML instances... 2019-10-11 13:51:35 Starting - Preparing the instances for training...... 2019-10-11 13:52:29 Downloading - Downloading input data 2019-10-11 13:52:29 Stopping - Stopping the training job 2019-10-11 13:52:29 Stopped - Training job stopped ..Training seconds: 1 Billable seconds: 1 training completed. Detective control explained The training job was terminated by an AWS Lambda function that was executed in response to a CloudWatch Event that was triggered when the training job was created. The Lambda function inspected the training job, saw that it was NOT attached to a VPC and stopped the training job from executing.\nAssume the role of the Data Science Administrator and review the code of the AWS Lambda function SagemakerTrainingJobVPCEnforcer. Also review the CloudWatch Event rule SagemakerTrainingJobVPCEnforcementRule and take note of the event which triggers execution of the Lambda function.\nStart a compliant training job To succesfully run your training job you will need to configure the training job to run within your VPC. To do this you will pass a collection of subnet IDs and security groups that we imported earlier.\nThe following sample code shows how these can be specified:\nTensorFlow(entry_point=\u0026#39;predictor.py\u0026#39;, ..., train_instance_count=1, train_instance_type=instance_type, subnets = [\u0026#39;subnet-0fc1ed6b334bd4cfd\u0026#39;,\u0026#39;subnet-0f398485e991f8333\u0026#39;], security_group_ids = [\u0026#39;sg-0da87d40633b8f922\u0026#39;], ... ) Execute the cell below the failed training job deployment titled Traing with a VPC, the training job should complete successfully, producing output similar to the following:\n2019-10-16 19:57:54 Starting - Starting the training job... 2019-10-16 19:57:56 Starting - Launching requested ML instances...... 2019-10-16 19:58:59 Starting - Preparing the instances for training... 2019-10-16 19:59:46 Downloading - Downloading input data... 2019-10-16 20:00:25 Training - Training image download completed. Training in progress.. 2019-10-16 20:00:25,711 INFO - root - running container entrypoint 2019-10-16 20:00:25,711 INFO - root - starting train task 2019-10-16 20:00:25,727 INFO - container_support.training - Training starting In this lab, you experienced a remediating detective control deployed by the cloud platform engineering team and reconfigured the SageMaker training job to run connected to your VPC. But waiting minutes to find out that your training job is going to error out is a slow and painful way to iterate during development.\nIn the next lab you will look into what preventive controls can be put in place to enhance your defense in depth and provide a better developer experience for the project team members.\n"
},
{
	"uri": "https://sagemaker-workshop.com/security_for_sysops.html",
	"title": "Building Secure Environments",
	"tags": [],
	"description": "",
	"content": "In this module we will introduce you to the recommended practices for building a secure data science environment powered by Amazon SageMaker. Like many other AWS services, Amazon SageMaker is secure by default. In these labs you will learn how to combine multiple secure by default services to enforce secure configurations and create a data science environment that meets common security requirements for many customers. You will cover various security topics and work through hands-on lab materials to exercise and explore the many security features available with Amazon Web Services.\nLet\u0026rsquo;s get started.\n"
},
{
	"uri": "https://sagemaker-workshop.com/security_for_sysops/best_practice.html",
	"title": "Codify Best Practice",
	"tags": [],
	"description": "",
	"content": "This lab introduces services to help you create a way for the project administration teams and the data science teams to self-service while adhering to desired, secure practices. In this lab you will learn about AWS Identity and Access Management as well as AWS Service Catalog. You will then use what you learned to enable your teams.\nLet\u0026rsquo;s begin.\n"
},
{
	"uri": "https://sagemaker-workshop.com/security_for_sysops/team_resources.html",
	"title": "Project Environment",
	"tags": [],
	"description": "",
	"content": "This lab will explore the tools available to you to create a secure environment, starting with the network. By establishing a secure network environment you can begin to use familiar IP-level controls to manage the flow of data and access to systems. You will learn some concepts and terms specific to AWS but should get a sense of how to create a private environment for your data science teams. You will also create resources to encrypt data as it flows from storage in Amazon S3, into the VPC on EBS volumes, and back out to Amazon S3.\nLet\u0026rsquo;s get started.\n"
},
{
	"uri": "https://sagemaker-workshop.com/security_for_sysops/secure_notebook.html",
	"title": "Working Securely",
	"tags": [],
	"description": "",
	"content": "This lab introduces some of the key security features of the Amazon SageMaker service and how you can use them to keep your data scientists operating in a secure manner. After introducing the concepts and features of the service this lab will then allow you to create a SageMaker Jupyter Notebook in a self-service manner.\nLet\u0026rsquo;s jump in.\n"
},
{
	"uri": "https://sagemaker-workshop.com/security_for_sysops/detective.html",
	"title": "Detection and Remediation",
	"tags": [],
	"description": "",
	"content": "This lab will briefly introduce the concept of detective and corrective controls. It will also introduce some of the AWS services which can be used to create detective and corrective controls. During the lab you will see these controls in action as they safeguard you and your environment, keeping you within your security policies.\nLet\u0026rsquo;s get to it.\n"
},
{
	"uri": "https://sagemaker-workshop.com/security_for_sysops/preventive.html",
	"title": "Prevention",
	"tags": [],
	"description": "",
	"content": "This lab will dive deeper into AWS Identity and Access Management and how it can be applied to Amazon SageMaker to create preventive controls. In the lab you will also update existing Service Catalog products to enforce your new best practice.\nLet\u0026rsquo;s dive in.\n"
},
{
	"uri": "https://sagemaker-workshop.com/security_for_sysops/preventive/preventive_controls.html",
	"title": "Preventive Controls",
	"tags": [],
	"description": "",
	"content": "Amazon SageMaker, like other services, is goverened by Identity and Access Management (IAM). You configure IAM using policy documents which explicitly grant permissions to your environment.\nAWS IAM is based upon the concepts of Principals, Actions, Resources, and Conditions (PARC). This allows you to specify, using IAM policies who (principals) can do what (actions) to which resources and under what circumstances (conditions). Conditions are a powerful part of IAM policies and gives you the ability to control aspects of the actions being taken by principals in your environment.\nUsing the language of principals, actions, resources, and conditions you have a rich language to configure preventive controls that govern your environment. Preventive controls can stop an action from ever succeeding. However, as part of Defense-in-Depth, you should also create corrective and detective controls.\nAmazon SageMaker provides numerous conditions that allow you to control the aspects of the many SageMaker actions your users will need. In this lab you will use some of these controls to stop data scientists from creating non-compliant training jobs. We won\u0026rsquo;t go into the full list of conditions which Amazon SageMaker makes available, but let\u0026rsquo;s take a look at some of the conditions available in the context of the machine learning lifecycle.\nAccessing a SageMaker notebook To access an Amazon SageMaker Jupyter notebook instance you need permissions to call CreatePresignedNotebookInstanceUrl. This action creates a pre-signed URL which grants access to the notebook instance. Both the SageMaker API call and the resulting URL are governed by the IAM conditions associated with the action. This means that you can leverage an IAM policy such as the following to restrict from which IP address someone in your environment can access the notebook instance.\n{ \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;sagemaker:StartNotebookInstance\u0026#34;, \u0026#34;sagemaker:StopNotebookInstance\u0026#34;, \u0026#34;sagemaker:DescribeNotebookInstance\u0026#34;, \u0026#34;sagemaker:CreatePresignedNotebookInstanceUrl\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;IpAddress\u0026#34;: { \u0026#34;aws:SourceIp\u0026#34;: [ \u0026#34;192.0.2.0/24\u0026#34;, \u0026#34;203.0.113.0/24\u0026#34; ] }, \u0026#34;StringLike\u0026#34;: { \u0026#34;sagemaker:ResourceTag/owner\u0026#34;: \u0026#34;${aws:userid}\u0026#34; } } } The snippet above will allow someone to start, stop, and access a SageMaker Jupyter notebook as long as they do so from a specific IP CIDR range and if the notebook has been tagged with their user as the owner.\nRequiring VPC attachment Actions for creating SageMaker resources such as notebooks, training jobs, and machine learning models can also have IAM conditions associated with them. This gives you the ability to require that principals create these resources with a VPC configuration.\n\u0026#34;Condition\u0026#34;: { \u0026#34;ForAllValues:StringEqualsIfExists\u0026#34;: { \u0026#34;sagemaker:VpcSubnets\u0026#34;: [ \u0026#34;subnet-0de4766fbf8d2ea38\u0026#34;, \u0026#34;subnet-0c974f191edc71a33\u0026#34; ], \u0026#34;sagemaker:VpcSecurityGroupIds\u0026#34;: [ \u0026#34;sg-02dc127e01d6b1407\u0026#34; ] } } The condition statement above specifies that IF an action has subnets and security groups configured the values provided must be within an approved set of values. In particular someone creating a Jupyter notebook instance, for example, would have to specify at least one of the subnets above and the security group specified. If any provided values were outside of the specified set the condition would evaluate to False and the permission be denied.\nAnother condition is to ensure that if a VPC configuration CAN be provided that it IS provided. You can do this using the Null condition.\n\u0026#34;Condition\u0026#34;: { \u0026#34;Null\u0026#34;: { \u0026#34;sagemaker:VpcSubnets\u0026#34;: \u0026#34;true\u0026#34; } } The condition above evaluates to True if there is no subnet configuration specified for an API call. This will enforce that any principal invoking the SageMaker action provides a VPC configuration and the previous condition will ensure that the configuration is within an acceptable set of values.\nA complete list of conditions and the SageMaker actions they are associated with can be found in the Amazon SageMaker documentation.\nVPC Endpoint policies In addition to associating IAM policies with users, groups, or roles you can also use the same policy language to create VPC endpoint policies. Endpoint policies control what actions can be taken using a VPC endpoint. Amazon SageMaker is accessible via VPC endpoints and so you can further enforce policies at the endpoint level to govern how users in your environments can access the SageMaker APIs. This adds to your ability to create defense in depth and ensure best practice in your environment.\nNow that you understand IAM policies and the conditions you can use to control user permissions, work through this lab to use IAM policy language to create a preventive control that will lead to an improved developer experience. Recall that as a data scientist you were waiting for the training job to start and eventually be stopped by the remediating detective control. On repeat this could be a slow and unpleasant way to discover you made an error. Use a preventive control to provide clear and immediate feedback to a developer to let them know when a mistake has been made.\n"
},
{
	"uri": "https://sagemaker-workshop.com/security_for_users.html",
	"title": "Using Secure Environments",
	"tags": [],
	"description": "",
	"content": "In this module you will be introduced to the recommended practices for using Amazon SageMaker in a secure data science environment. Like many other AWS services, Amazon SageMaker is secure by default. Throughout this workshop you will see how you can work in a secured data science environment. You will cover the many stages of the machine learning lifecycle and be provided with Jupyter notebooks to step through that lifecycle while maintaining a high bar for security.\nLet\u0026rsquo;s get started.\n"
},
{
	"uri": "https://sagemaker-workshop.com/security_for_users/environment.html",
	"title": "Deploy a Secure Environment",
	"tags": [],
	"description": "",
	"content": "In this module you will use AWS services to quickly deploy a secure environment for a data science project. After deploying the environment you will also create a SageMaker notebook for use throughout the remainder of this module.\nPlease know that this module creates the same environment that was created in the Building Secure Environments module. If you have worked through that module already and have a secured data science environment, you can reuse that environment here and can skip ahead to Secure Data Science Notebooks.\nLet\u0026rsquo;s get building.\n"
},
{
	"uri": "https://sagemaker-workshop.com/security_for_sysops/preventive/preventive_lab.html",
	"title": "Lab 5: Preventive Controls",
	"tags": [],
	"description": "",
	"content": "In this lab, you will implement a preventive control that will stop a training job from starting if it\u0026rsquo;s not launched within a VPC. In the interest of defence in depth you will implement the preventive control to complement the detective control exercised in the previous lab.\nThe preventive control you will deploy here is a modification to the IAM policy which grants a user\u0026rsquo;s notebook the permission to launch a training job. Every user\u0026rsquo;s notebook has an IAM role created for it and an IAM policy attached to this role. Updating these policies individually is not feasible at scale so use the ability of AWS Service Catalog to do the heavy lifting for you.\nCreate an updated version of the Jupyter notebook product to deliver the updated IAM policy to user\u0026rsquo;s notebooks. Resume the role of the Project Administration team and visit the AWS Service Catalog console. Drill into the SageMaker notebook product and provide an updated teamplate for this product using the appropriate URL from the list below. When you have a new version of the SageMaker notebook product created return to the role of the project team member and update your notebook to the latest revision.\nIreland (eu-west-1)\nhttps://s3.eu-west-1.amazonaws.com/sagemaker-workshop-cloudformation-eu-west-1/quickstart/ds_notebook_v2.yaml\nLondon (eu-west-2)\nhttps://s3.eu-west-2.amazonaws.com/sagemaker-workshop-cloudformation-eu-west-2/quickstart/ds_notebook_v2.yaml\nSydney (ap-southeast-2)\nhttps://s3.ap-southeast-2.amazonaws.com/sagemaker-workshop-cloudformation-ap-southeast-2/quickstart/ds_notebook_v2.yaml\nOregon (us-west-2)\nhttps://s3.us-west-2.amazonaws.com/sagemaker-workshop-cloudformation-us-west-2/quickstart/ds_notebook_v2.yaml\nN. Virginia (us-east-1)\nhttps://s3.us-east-1.amazonaws.com/sagemaker-workshop-cloudformation-us-east-1/quickstart/ds_notebook_v2.yaml\nOhio (us-east-2)\nhttps://s3.us-east-2.amazonaws.com/sagemaker-workshop-cloudformation-us-east-2/quickstart/ds_notebook_v2.yaml\nStep-by-step instructions\rThe preventive control will be a modified IAM policy associated with the exeuction role of the SageMaker notebook instance. To modify the role you first need to assume the role of the Data Science Administrator. Next click Products under Administration on the left-hand navigation bar. Click the link for the SageMaker notebook product you want to update. On the product detail page click Create new version. Click Use a CloudFormation Template Paste the appropriate URL for your region below into the CloudFormation template field. Type Version 2 for the Version title. Click Create product version. Navigate to Provisioned products Resume the role of a data science project team member and navigate to Service Catalog\u0026rsquo;s Provisioned Products console Click the context menu next to your notebook and select Update provisioned product Click the radio button next to Version 2 and click Next. Click Next and then Update and wait for the notebook\u0026rsquo;s permissions to be updated. After the product has been successfully updated revist the Jupyter notebook kernel and execute the cell titled Train Without VPC Configured. You should now quickly receive an Access Denied exception similar to the below:\nClientError: An error occurred (AccessDeniedException) when calling the CreateTrainingJob operation: User: arn:aws:sts::012348485732:assumed-role/SageMakerExecRole-ml-product-team/SageMaker is not authorized to perform: sagemaker:CreateTrainingJob on resource: arn:aws:sagemaker:eu-west-1:012348485732:training-job/sagemaker-tensorflow-2019-10-16-22-14-30-880 with an explicit deny In this lab you modified the permissions granted to the instances of the data science team\u0026rsquo;s Jupyter notebooks. This altered their permissions so that they could only perform actions like creating a training job if that action met the security requirement of specifying a VPC configuration. Visit the defined products using the Service Catalog console to review the CloudFormation template and the changes it made to the permissions. Or use the IAM console to review the role you created and the policy attached to it. What conditions are on the IAM policy controlling access to the SageMaker API?\n"
},
{
	"uri": "https://sagemaker-workshop.com/security_for_users/notebook.html",
	"title": "Secure Data Science Notebooks",
	"tags": [],
	"description": "",
	"content": "You are now connected to a Jupyter notebook server running within a secured data science environment. From the notebook you have access to a centralized, hosted PyPI mirror and select buckets in Amazon S3. There is no open access to the Internet or unnecessary AWS services to safeguard against data exfiltration. Using this environment you will now work through the various stages of the machine learning lifecycle, from data exploration, through training, and finally to hosting and monitoring.\nLet\u0026rsquo;s get started.\n"
},
{
	"uri": "https://sagemaker-workshop.com/cleanup.html",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "To avoid charges for endpoints and other resources you might not need after the workshop, you need to delete them or stop them. In this module you\u0026rsquo;ll work through the resources created in the workshop and remove them.\n"
},
{
	"uri": "https://sagemaker-workshop.com/conclusion.html",
	"title": "Conclusion",
	"tags": [],
	"description": "",
	"content": "What Have We Accomplished? "
},
{
	"uri": "https://sagemaker-workshop.com/security_for_sysops/summary.html",
	"title": "Summary",
	"tags": [],
	"description": "",
	"content": "Let\u0026rsquo;s look back on what you\u0026rsquo;ve accomplished during the labs in this section.\nIn this series of labs you used many AWS services to support your machine learning process. As the Cloud Platform Engineering team you created a secure network environment with corrective controls for the data science administration team. You also created a self-service product that codified best practices and allowed the Data Science Administration team to support the data science project teams without needing expansive permissions in AWS.\nAs the Data Science Administration team you used the enviornment and product created by the CPE to support the Data Science project team. You created an IAM role for the data science team, an encryption key, and Amazon S3 buckets for the project team to store their data and models. Finally you created a self-service product that the data science team could use to provision Jupyter notebooks for themselves, also without needing extensive permissions into AWS.\nThen, as a data scientist and a member of the project team, you created a Jupyter notebook instance and trained a machine learning model in accordance with your security best practice. This demonstrated the corrective control deployed and managed by the Cloud Platform Engineering team as well as the preventive control later deployed by the Data Science Administration team.\nThis module relied on the following key services that you should now be familiar with:\nAWS Service Catalog AWS Key Management Service (KMS) Amazon S3 Amazon SageMaker AWS Identity and Access Management (IAM) AWS CloudFormation Amazon Virtual Private Cloud (VPC) To find out more about any of these services please visit their documentation or visit AWS Security Workshops for more great security-focused workshops.\n"
},
{
	"uri": "https://sagemaker-workshop.com/security_for_users/summary.html",
	"title": "Summary",
	"tags": [],
	"description": "",
	"content": "Let\u0026rsquo;s look back on what you\u0026rsquo;ve accomplished during the labs in this section.\nIn this series of labs you used many AWS services to support your machine learning process. As the Cloud Platform Engineering team you created a secure network environment with corrective controls for the data science administration team. You also created a self-service product that codified best practices and allowed the Data Science Administration team to support the data science project teams without needing expansive permissions in AWS.\nAs the Data Science Administration team you used the enviornment and product created by the CPE to support the Data Science project team. You created an IAM role for the data science team, an encryption key, and Amazon S3 buckets for the project team to store their data and models. Finally you created a self-service product that the data science team could use to provision Jupyter notebooks for themselves, also without needing extensive permissions into AWS.\nThen, as a data scientist and a member of the project team, you created a Jupyter notebook instance and trained a machine learning model in accordance with your security best practice. This demonstrated the corrective control deployed and managed by the Cloud Platform Engineering team as well as the preventive control later deployed by the Data Science Administration team.\nThis module relied on the following key services that you should now be familiar with:\nAWS Service Catalog AWS Key Management Service (KMS) Amazon S3 Amazon SageMaker AWS Identity and Access Management (IAM) AWS CloudFormation Amazon Virtual Private Cloud (VPC) To find out more about any of these services please visit their documentation or visit AWS Security Workshops for more great security-focused workshops.\n"
},
{
	"uri": "https://sagemaker-workshop.com/security_for_sysops/faq.html",
	"title": "Frequently Asked Questions",
	"tags": [],
	"description": "",
	"content": "The following is a compiled list of questions or issues you may face while going through these labs. If you encounter any issues, hopefully you will find helpful guidance below.\nQ: I am trying to execute a cell in Jupyter Notebook but nothing happens.\nA: Check whether the kernel has finished loading. You can see kernel status in the upper right corner of the Notebook. For the purpose of this workshop, the kernel should be conda_tensorflow_p27. If the kernel has not loaded, give it a couple of minutes to load. If it still says No Kernel, then navigate to the Jupyter menu bar and select Kernel  Restart Kernel. Select conda_tensorflow_p27 if prompted for the kernel type.\nQ: Training job completes successfully, but I see all sorts of errors and warnings in the logs such as No response body. Response code: 404 and If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\nA: You may ignore these informational log messages for the purpose of this workshop.\nQ: Training job or hosting endpoint deployment terminate in error ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateEndpoint operation: The account-level service limit \u0026lsquo;ml.m4.xlarge for endpoint usage\u0026rsquo; is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit.\nA: Try to launch a new training job with another instance type (see SageMaker instance types https://aws.amazon.com/sagemaker/pricing/instance-types/). If error persists, contact workshop support staff or your AWS account team.\nQ: In Lab 2, we use AWS Service Catalog to create a team IAM role and CloudFormation to deploy the rest of the team resources including S3 buckets, KMS key, etc. Why not do everything in Service Catalog?\nA: One reason is educational, to get exposure to both services. Another reason is that creating an IAM role requires privileged permissions to IAM service. Service Catalog provides a great option for delegating IAM role creation in a safe manner without having to grant the Data Science Admin permission to the underlying IAM service.\n"
},
{
	"uri": "https://sagemaker-workshop.com/security_for_users/faq.html",
	"title": "Frequently Asked Questions",
	"tags": [],
	"description": "",
	"content": "The following is a compiled list of questions or issues you may face while going through these labs. If you encounter any issues, hopefully you will find helpful guidance below.\nQ: I am trying to execute a cell in Jupyter Notebook but nothing happens.\nA: Check whether the kernel has finished loading. You can see kernel status in the upper right corner of the Notebook. For the purpose of this workshop, the kernel should be conda_tensorflow_p27. If the kernel has not loaded, give it a couple of minutes to load. If it still says No Kernel, then navigate to the Jupyter menu bar and select Kernel  Restart Kernel. Select conda_tensorflow_p27 if prompted for the kernel type.\nQ: Training job completes successfully, but I see all sorts of errors and warnings in the logs such as No response body. Response code: 404 and If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\nA: You may ignore these informational log messages for the purpose of this workshop.\nQ: Training job or hosting endpoint deployment terminate in error ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateEndpoint operation: The account-level service limit \u0026lsquo;ml.m4.xlarge for endpoint usage\u0026rsquo; is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit.\nA: Try to launch a new training job with another instance type (see SageMaker instance types https://aws.amazon.com/sagemaker/pricing/instance-types/). If error persists, contact workshop support staff or your AWS account team.\nQ: In Lab 2, we use AWS Service Catalog to create a team IAM role and CloudFormation to deploy the rest of the team resources including S3 buckets, KMS key, etc. Why not do everything in Service Catalog?\nA: One reason is educational, to get exposure to both services. Another reason is that creating an IAM role requires privileged permissions to IAM service. Service Catalog provides a great option for delegating IAM role creation in a safe manner without having to grant the Data Science Admin permission to the underlying IAM service.\n"
},
{
	"uri": "https://sagemaker-workshop.com/categories.html",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://sagemaker-workshop.com/tags.html",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]